{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from contextlib import contextmanager\n",
    "import sys\n",
    "sys.path.append('../../../code/utils')\n",
    "from perf_utils import get_memory_str, get_memory_bytes, format_memory_diff, format_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIR = '../../../data/split/preliminary_contest_data/byUserFeatureName/'\n",
    "VOCAB_DIR = '../../../data/vocabulary/preliminary_contest_data/'\n",
    "\n",
    "\n",
    "def feature_path(feat_name):\n",
    "    filename = \"userFeature.[featureName='{}'].data\".format(feat_name)\n",
    "    return os.path.join(FEATURE_DIR, filename)\n",
    "\n",
    "\n",
    "def vocab_path(feat_name=\"all\"):\n",
    "    if feat_name == \"all\":\n",
    "        filename = \"userFeature.pkl\"\n",
    "    else:\n",
    "        filename = \"userFeature.[featureName='{}'].pkl\".format(feat_name)\n",
    "    return os.path.join(VOCAB_DIR, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    obj = None\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def save_as_pickle(obj, filepath):\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(feat_name, **kw):\n",
    "    sep = kw.pop('sep', '|')\n",
    "    dtype = kw.pop('dtype', {feat_name: str})\n",
    "    filepath = feature_path(feat_name)\n",
    "    return pd.read_csv(filepath, sep=sep, dtype=dtype, **kw)\n",
    "\n",
    "def load_vocab(feat_name='all'):\n",
    "    filepath = vocab_path(feat_name)\n",
    "    return load_pickle(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_str():\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def profiler(task_name, verbose_memory=True, verbose_time=True):\n",
    "    t0 = time.time()\n",
    "    m0 = get_memory_bytes()\n",
    "    yield\n",
    "    t_delta = time.time() - t0\n",
    "    m_delta = get_memory_bytes() - m0\n",
    "    msg = \"[{}] Finish {}.\".format(get_time_str(), task_name)\n",
    "    if verbose_memory:\n",
    "        msg += \" △M: {}.\".format(format_memory_diff(m_delta))\n",
    "    if verbose_time:\n",
    "        msg += \" △T: {}.\".format(format_secs(t_delta))\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize(series, vocab):\n",
    "    # My implementation of CountVectorizer FOR THIS CASE ONLY\n",
    "    # see https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "    # for the standard CSR representation.\n",
    "    # It's faster since:\n",
    "    # 1. There is no checking in each iteration, because I assume that the input text and vocabulary matches\n",
    "    # 2. Technically I didn't use iteration, I use pandas apply instead. That's why the input text must be pd.Series\n",
    "    # 3. I use library like `itertools` instead of starting from scratch by myself\n",
    "    # 4. It will be much faster theoretically if I add multiprocessing\n",
    "    vocab_map = {val: i for i, val in enumerate(vocab)}  # mapping word(str) to column index(int)\n",
    "    lst_series = series.apply(lambda x: [vocab_map[val] for val in x.split()])  # pd.Series with each row: list of int\n",
    "    cnt_series = lst_series.apply(len)  # pd.Series with each row: int, indicating the number of words\n",
    "    indptr = np.concatenate((np.zeros(1), np.add.accumulate(cnt_series)))  # there should be a zeros at the beginning\n",
    "    indices = list(itertools.chain.from_iterable(lst_series))  # to concatenate lists from all rows\n",
    "    data = np.ones(len(indices), dtype=np.int8)  # all non-zero value is one.\n",
    "    cnt_vec = csr_matrix((data, indices, indptr), dtype=np.int8)  # see the link above for detailed explanation\n",
    "    \n",
    "    # clean memory. I am not sure whether they work. to be checked\n",
    "    vocab_map.clear()\n",
    "    del [[lst_series, cnt_series]]\n",
    "    del indptr\n",
    "    del indices\n",
    "    del data\n",
    "    gc.collect()\n",
    "    lst_series = pd.DataFrame()  # not sure whether it work\n",
    "    cnt_series = pd.DataFrame()  # same\n",
    "\n",
    "    return cnt_vec, vocab_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_dir = '../../../data/nlp_count/preliminary_contest_data/byUserFeatureName/'\n",
    "tfidf_dir = '../../../data/nlp_tfidf/preliminary_contest_data/byUserFeatureName/'\n",
    "os.makedirs(cnt_dir, exist_ok=True)\n",
    "os.makedirs(tfidf_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage at this moment: 91.34MB\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'age'...\n",
      "[04:12:01] Finish loading and preprocessing. △M: +148.76MB. △T: 1.9 seconds.\n",
      "[04:12:21] Finish count vectorizing (ngram=1) [sklearn]. △M: +83.05MB. △T: 19.9 seconds.\n",
      "[04:12:33] Finish count vectorizing (ngram=1) [Janzen's]. △M: +600.78MB. △T: 12.4 seconds.\n",
      "[04:12:33] Finish saving count vectors. △M: -221.69MB.\n",
      "[04:12:53] Finish TFIDF vectorizing (ngram=1) transformation. △M: +84.0KB. △T: 19.7 seconds.\n",
      "[04:12:54] Finish saving TFIDF vectors. △M: +16.0KB.\n",
      "[04:12:54] Finish cleaning. △M: -147.81MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'gender'...\n",
      "[04:12:55] Finish loading and preprocessing. △M: +4.0KB. △T: 1.5 seconds.\n",
      "[04:13:15] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 19.8 seconds.\n",
      "[04:13:28] Finish count vectorizing (ngram=1) [Janzen's]. △M: +148.71MB. △T: 12.9 seconds.\n",
      "[04:13:28] Finish saving count vectors. △M: -148.11MB.\n",
      "[04:13:48] Finish TFIDF vectorizing (ngram=1) transformation. △M: +73.91MB. △T: 19.7 seconds.\n",
      "[04:13:48] Finish saving TFIDF vectors. △M: -73.9MB.\n",
      "[04:13:48] Finish cleaning. △M: +0B.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'marriageStatus'...\n",
      "[04:13:50] Finish loading and preprocessing. △M: +0B. △T: 1.7 seconds.\n",
      "[04:14:10] Finish count vectorizing (ngram=1) [sklearn]. △M: +12.0KB. △T: 20.0 seconds.\n",
      "[04:14:23] Finish count vectorizing (ngram=1) [Janzen's]. △M: +162.67MB. △T: 13.2 seconds.\n",
      "[04:14:23] Finish saving count vectors. △M: -162.07MB.\n",
      "[04:14:44] Finish TFIDF vectorizing (ngram=1) transformation. △M: +88.17MB. △T: 21.0 seconds.\n",
      "[04:14:45] Finish saving TFIDF vectors. △M: -88.51MB.\n",
      "[04:14:45] Finish cleaning. △M: -256.0KB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'education'...\n",
      "[04:14:47] Finish loading and preprocessing. △M: +0B. △T: 1.5 seconds.\n",
      "[04:15:06] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 19.8 seconds.\n",
      "[04:15:19] Finish count vectorizing (ngram=1) [Janzen's]. △M: +148.12MB. △T: 12.7 seconds.\n",
      "[04:15:19] Finish saving count vectors. △M: -147.81MB.\n",
      "[04:15:39] Finish TFIDF vectorizing (ngram=1) transformation. △M: +73.91MB. △T: 20.1 seconds.\n",
      "[04:15:40] Finish saving TFIDF vectors. △M: -73.91MB.\n",
      "[04:15:40] Finish cleaning. △M: +0B.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'consumptionAbility'...\n",
      "[04:15:41] Finish loading and preprocessing. △M: +0B. △T: 1.6 seconds.\n",
      "[04:16:01] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 19.9 seconds.\n",
      "[04:16:14] Finish count vectorizing (ngram=1) [Janzen's]. △M: +148.06MB. △T: 12.7 seconds.\n",
      "[04:16:14] Finish saving count vectors. △M: -147.87MB.\n",
      "[04:16:34] Finish TFIDF vectorizing (ngram=1) transformation. △M: +73.91MB. △T: 19.6 seconds.\n",
      "[04:16:34] Finish saving TFIDF vectors. △M: -73.9MB.\n",
      "[04:16:34] Finish cleaning. △M: -512.0KB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'LBS'...\n",
      "[04:16:36] Finish loading and preprocessing. △M: +340.0KB. △T: 2.2 seconds.\n",
      "[04:16:56] Finish count vectorizing (ngram=1) [sklearn]. △M: +36.97MB. △T: 19.6 seconds.\n",
      "[04:17:09] Finish count vectorizing (ngram=1) [Janzen's]. △M: +471.98MB. △T: 12.6 seconds.\n",
      "[04:17:09] Finish saving count vectors. △M: -508.52MB.\n",
      "[04:17:29] Finish TFIDF vectorizing (ngram=1) transformation. △M: +110.86MB. △T: 20.3 seconds.\n",
      "[04:17:30] Finish saving TFIDF vectors. △M: -110.86MB.\n",
      "[04:17:30] Finish cleaning. △M: -512.0KB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest1'...\n",
      "[04:17:37] Finish loading and preprocessing. △M: +674.39MB. △T: 7.5 seconds.\n",
      "[04:18:45] Finish count vectorizing (ngram=1) [sklearn]. △M: +609.58MB. △T: 1.1 minutes.\n",
      "[04:19:32] Finish count vectorizing (ngram=1) [Janzen's]. △M: +2.45GB. △T: 47.4 seconds.\n",
      "[04:19:34] Finish saving count vectors. △M: -1.74GB.\n",
      "[04:20:43] Finish TFIDF vectorizing (ngram=1) transformation. △M: +780.84MB. △T: 1.2 minutes.\n",
      "[04:20:47] Finish saving TFIDF vectors. △M: -1.43GB.\n",
      "[04:20:47] Finish cleaning. △M: -670.25MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest2'...\n",
      "[04:20:51] Finish loading and preprocessing. △M: +170.74MB. △T: 3.7 seconds.\n",
      "[04:21:22] Finish count vectorizing (ngram=1) [sklearn]. △M: +149.57MB. △T: 31.0 seconds.\n",
      "[04:21:46] Finish count vectorizing (ngram=1) [Janzen's]. △M: +373.67MB. △T: 23.8 seconds.\n",
      "[04:21:46] Finish saving count vectors. △M: -522.98MB.\n",
      "[04:22:19] Finish TFIDF vectorizing (ngram=1) transformation. △M: +8.0KB. △T: 32.3 seconds.\n",
      "[04:22:20] Finish saving TFIDF vectors. △M: +0B.\n",
      "[04:22:20] Finish cleaning. △M: -173.5MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest3'...\n",
      "[04:22:22] Finish loading and preprocessing. △M: -508.0KB. △T: 1.8 seconds.\n",
      "[04:22:42] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 19.7 seconds.\n",
      "[04:22:54] Finish count vectorizing (ngram=1) [Janzen's]. △M: +86.75MB. △T: 12.1 seconds.\n",
      "[04:22:54] Finish saving count vectors. △M: -86.25MB.\n",
      "[04:23:14] Finish TFIDF vectorizing (ngram=1) transformation. △M: -252.0KB. △T: 19.9 seconds.\n",
      "[04:23:14] Finish saving TFIDF vectors. △M: +0B.\n",
      "[04:23:14] Finish cleaning. △M: -1.25MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest4'...\n",
      "[04:23:16] Finish loading and preprocessing. △M: +0B. △T: 1.7 seconds.\n",
      "[04:23:35] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 19.1 seconds.\n",
      "[04:23:46] Finish count vectorizing (ngram=1) [Janzen's]. △M: +152.26MB. △T: 11.7 seconds.\n",
      "[04:23:47] Finish saving count vectors. △M: -151.76MB.\n",
      "[04:24:06] Finish TFIDF vectorizing (ngram=1) transformation. △M: +0B. △T: 19.7 seconds.\n",
      "[04:24:07] Finish saving TFIDF vectors. △M: -256.0KB.\n",
      "[04:24:07] Finish cleaning. △M: -512.0KB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest5'...\n",
      "[04:24:15] Finish loading and preprocessing. △M: +773.39MB. △T: 8.1 seconds.\n",
      "[04:25:27] Finish count vectorizing (ngram=1) [sklearn]. △M: +558.09MB. △T: 1.2 minutes.\n",
      "[04:26:20] Finish count vectorizing (ngram=1) [Janzen's]. △M: +2.26GB. △T: 52.2 seconds.\n",
      "[04:26:21] Finish saving count vectors. △M: -2.81GB.\n",
      "[04:27:39] Finish TFIDF vectorizing (ngram=1) transformation. △M: +1.09GB. △T: 1.3 minutes.\n",
      "[04:27:44] Finish saving TFIDF vectors. △M: -1.09GB.\n",
      "[04:27:44] Finish cleaning. △M: -771.0MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'kw1'...\n",
      "[04:27:52] Finish loading and preprocessing. △M: +729.51MB. △T: 7.5 seconds.\n",
      "[04:28:29] Finish count vectorizing (ngram=1) [sklearn]. △M: +8.04MB. △T: 36.8 seconds.\n",
      "[04:29:04] Finish count vectorizing (ngram=1) [Janzen's]. △M: +618.47MB. △T: 35.6 seconds.\n",
      "[04:29:05] Finish saving count vectors. △M: -625.77MB.\n",
      "[04:29:46] Finish TFIDF vectorizing (ngram=1) transformation. △M: +334.22MB. △T: 40.7 seconds.\n",
      "[04:29:47] Finish saving TFIDF vectors. △M: -334.05MB.\n",
      "[04:29:47] Finish cleaning. △M: -728.05MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'kw2'...\n",
      "[04:29:55] Finish loading and preprocessing. △M: +663.89MB. △T: 7.5 seconds.\n",
      "[04:30:30] Finish count vectorizing (ngram=1) [sklearn]. △M: +178.14MB. △T: 35.0 seconds.\n",
      "[04:30:59] Finish count vectorizing (ngram=1) [Janzen's]. △M: +499.38MB. △T: 28.9 seconds.\n",
      "[04:31:00] Finish saving count vectors. △M: -676.87MB.\n",
      "[04:31:37] Finish TFIDF vectorizing (ngram=1) transformation. △M: +354.13MB. △T: 37.2 seconds.\n",
      "[04:31:38] Finish saving TFIDF vectors. △M: -353.77MB.\n",
      "[04:31:39] Finish cleaning. △M: -665.65MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'kw3'...\n",
      "[04:31:41] Finish loading and preprocessing. △M: +27.76MB. △T: 2.0 seconds.\n",
      "[04:32:01] Finish count vectorizing (ngram=1) [sklearn]. △M: +356.0KB. △T: 20.0 seconds.\n",
      "[04:32:13] Finish count vectorizing (ngram=1) [Janzen's]. △M: +162.11MB. △T: 12.7 seconds.\n",
      "[04:32:13] Finish saving count vectors. △M: -161.21MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:32:34] Finish TFIDF vectorizing (ngram=1) transformation. △M: +0B. △T: 20.4 seconds.\n",
      "[04:32:34] Finish saving TFIDF vectors. △M: -256.0KB.\n",
      "[04:32:34] Finish cleaning. △M: -30.75MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'topic1'...\n",
      "[04:32:41] Finish loading and preprocessing. △M: +655.29MB. △T: 6.9 seconds.\n",
      "[04:33:14] Finish count vectorizing (ngram=1) [sklearn]. △M: +173.13MB. △T: 33.4 seconds.\n",
      "[04:33:41] Finish count vectorizing (ngram=1) [Janzen's]. △M: +481.62MB. △T: 26.3 seconds.\n",
      "[04:33:41] Finish saving count vectors. △M: -654.05MB.\n",
      "[04:34:17] Finish TFIDF vectorizing (ngram=1) transformation. △M: +345.67MB. △T: 35.4 seconds.\n",
      "[04:34:18] Finish saving TFIDF vectors. △M: -345.67MB.\n",
      "[04:34:19] Finish cleaning. △M: -654.25MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'topic2'...\n",
      "[04:34:25] Finish loading and preprocessing. △M: +492.36MB. △T: 6.0 seconds.\n",
      "[04:34:58] Finish count vectorizing (ngram=1) [sklearn]. △M: +300.0KB. △T: 33.7 seconds.\n",
      "[04:35:25] Finish count vectorizing (ngram=1) [Janzen's]. △M: +432.95MB. △T: 26.9 seconds.\n",
      "[04:35:26] Finish saving count vectors. △M: -432.34MB.\n",
      "[04:36:02] Finish TFIDF vectorizing (ngram=1) transformation. △M: +358.17MB. △T: 36.1 seconds.\n",
      "[04:36:03] Finish saving TFIDF vectors. △M: -358.17MB.\n",
      "[04:36:04] Finish cleaning. △M: -494.25MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'topic3'...\n",
      "[04:36:06] Finish loading and preprocessing. △M: +28.78MB. △T: 2.0 seconds.\n",
      "[04:36:26] Finish count vectorizing (ngram=1) [sklearn]. △M: +168.0KB. △T: 19.9 seconds.\n",
      "[04:36:38] Finish count vectorizing (ngram=1) [Janzen's]. △M: +162.29MB. △T: 12.6 seconds.\n",
      "[04:36:38] Finish saving count vectors. △M: -161.48MB.\n",
      "[04:36:59] Finish TFIDF vectorizing (ngram=1) transformation. △M: +0B. △T: 20.6 seconds.\n",
      "[04:36:59] Finish saving TFIDF vectors. △M: +0B.\n",
      "[04:36:59] Finish cleaning. △M: -29.75MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'appIdInstall'...\n",
      "[04:37:02] Finish loading and preprocessing. △M: +21.82MB. △T: 2.9 seconds.\n",
      "[04:37:33] Finish count vectorizing (ngram=1) [sklearn]. △M: +2.04MB. △T: 30.3 seconds.\n",
      "[04:37:54] Finish count vectorizing (ngram=1) [Janzen's]. △M: +318.32MB. △T: 21.2 seconds.\n",
      "[04:37:54] Finish saving count vectors. △M: -319.43MB.\n",
      "[04:38:26] Finish TFIDF vectorizing (ngram=1) transformation. △M: +244.39MB. △T: 32.0 seconds.\n",
      "[04:38:27] Finish saving TFIDF vectors. △M: -244.27MB.\n",
      "[04:38:27] Finish cleaning. △M: -22.0MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'appIdAction'...\n",
      "[04:38:29] Finish loading and preprocessing. △M: +9.97MB. △T: 1.9 seconds.\n",
      "[04:38:50] Finish count vectorizing (ngram=1) [sklearn]. △M: +208.0KB. △T: 20.1 seconds.\n",
      "[04:39:02] Finish count vectorizing (ngram=1) [Janzen's]. △M: +84.8MB. △T: 12.5 seconds.\n",
      "[04:39:02] Finish saving count vectors. △M: -84.09MB.\n",
      "[04:39:22] Finish TFIDF vectorizing (ngram=1) transformation. △M: +4.0KB. △T: 20.1 seconds.\n",
      "[04:39:23] Finish saving TFIDF vectors. △M: +0B.\n",
      "[04:39:23] Finish cleaning. △M: -11.25MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'ct'...\n",
      "[04:39:24] Finish loading and preprocessing. △M: -256.0KB. △T: 1.8 seconds.\n",
      "[04:39:45] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 20.9 seconds.\n",
      "[04:39:59] Finish count vectorizing (ngram=1) [Janzen's]. △M: +74.38MB. △T: 13.8 seconds.\n",
      "[04:39:59] Finish saving count vectors. △M: -74.13MB.\n",
      "[04:40:20] Finish TFIDF vectorizing (ngram=1) transformation. △M: +0B. △T: 21.2 seconds.\n",
      "[04:40:21] Finish saving TFIDF vectors. △M: -512.0KB.\n",
      "[04:40:21] Finish cleaning. △M: -768.0KB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'os'...\n",
      "[04:40:23] Finish loading and preprocessing. △M: +0B. △T: 1.5 seconds.\n",
      "[04:40:42] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 19.3 seconds.\n",
      "[04:40:54] Finish count vectorizing (ngram=1) [Janzen's]. △M: +154.07MB. △T: 11.8 seconds.\n",
      "[04:40:54] Finish saving count vectors. △M: -153.57MB.\n",
      "[04:41:14] Finish TFIDF vectorizing (ngram=1) transformation. △M: -256.0KB. △T: 19.8 seconds.\n",
      "[04:41:14] Finish saving TFIDF vectors. △M: +0B.\n",
      "[04:41:14] Finish cleaning. △M: -512.0KB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'carrier'...\n",
      "[04:41:15] Finish loading and preprocessing. △M: +0B. △T: 1.5 seconds.\n",
      "[04:41:35] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 19.0 seconds.\n",
      "[04:41:46] Finish count vectorizing (ngram=1) [Janzen's]. △M: +344.0KB. △T: 11.6 seconds.\n",
      "[04:41:46] Finish saving count vectors. △M: -88.0KB.\n",
      "[04:42:06] Finish TFIDF vectorizing (ngram=1) transformation. △M: +0B. △T: 20.2 seconds.\n",
      "[04:42:07] Finish saving TFIDF vectors. △M: +0B.\n",
      "[04:42:07] Finish cleaning. △M: +0B.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'house'...\n",
      "[04:42:09] Finish loading and preprocessing. △M: +0B. △T: 1.9 seconds.\n",
      "[04:42:28] Finish count vectorizing (ngram=1) [sklearn]. △M: +0B. △T: 19.2 seconds.\n",
      "[04:42:39] Finish count vectorizing (ngram=1) [Janzen's]. △M: +596.0KB. △T: 11.7 seconds.\n",
      "[04:42:40] Finish saving count vectors. △M: -84.0KB.\n",
      "[04:42:59] Finish TFIDF vectorizing (ngram=1) transformation. △M: +0B. △T: 19.8 seconds.\n",
      "[04:43:00] Finish saving TFIDF vectors. △M: +0B.\n",
      "[04:43:00] Finish cleaning. △M: -256.0KB.\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(string):\n",
    "    return string.split()\n",
    "\n",
    "\n",
    "feat_names = [\"age\", \"gender\", \"marriageStatus\", \"education\", \"consumptionAbility\", \"LBS\",\n",
    "              \"interest1\", \"interest2\", \"interest3\", \"interest4\", \"interest5\",\n",
    "              \"kw1\", \"kw2\", \"kw3\", \"topic1\", \"topic2\", \"topic3\", \"appIdInstall\",\n",
    "              \"appIdAction\", \"ct\", \"os\", \"carrier\", \"house\"]\n",
    "\n",
    "print(\"Memory usage at this moment: {}\".format(get_memory_str()))\n",
    "for i, feat_name in enumerate(feat_names):\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Processing '{}'...\".format(feat_name))\n",
    "    with profiler(\"loading and preprocessing\"):\n",
    "        df = load_feature(feat_name)  # pd.DataFrame\n",
    "        vocab = load_vocab(feat_name)  # list\n",
    "        docs = df[feat_name]  # pd.Series\n",
    "        if docs.isnull().sum() > 0:\n",
    "            fill_value = \"[nan]\"  # don't use [NaN]; fxxk sklearn\n",
    "            vocab += [fill_value]\n",
    "            docs = docs.fillna(fill_value)\n",
    "        \n",
    "    with profiler(\"count vectorizing (ngram=1) [sklearn]\"):\n",
    "        cnt_vectorizer = CountVectorizer(vocabulary=vocab, \n",
    "                                         tokenizer=tokenizer,\n",
    "                                         dtype=np.int8)\n",
    "        cnt_vec_sk = cnt_vectorizer.fit_transform(docs)\n",
    "        checksum_sk = cnt_vec_sk.getnnz()\n",
    "        \n",
    "    with profiler(\"count vectorizing (ngram=1) [Janzen's]\"):\n",
    "        # My method is about 60% faster than sklearn (though it is only applicable for this case)\n",
    "        # Just forget the memory usage, Jupyter's memory mechanism sucks. : )\n",
    "        # There must be some dirty py deal between sklearn/scipy/numpy/pandas and Jupyter, fxxk them all\n",
    "        # I am faster and I haven't add multiprocessing and cython yet. : )\n",
    "        # I believe if I add 4-way multiprocessing, the speed will be at least doubled compared with this version\n",
    "        # Fxxk Jupyter Notebook\n",
    "        cnt_vec_jz, cnt_dict = count_vectorize(docs, vocab)\n",
    "        checksum_jz = cnt_vec_jz.getnnz()\n",
    "        \n",
    "    assert checksum_jz == checksum_sk\n",
    "    \n",
    "    with profiler(\"saving count vectors\", verbose_time=False):\n",
    "        cnt_file = \"userFeature.[featureName='{}'].pkl\".format(feat_name)\n",
    "        cnt_path = os.path.join(cnt_dir, cnt_file)\n",
    "        # save_as_pickle((cnt_dict, cnt_vec_jz), cnt_path)\n",
    "        save_as_pickle((cnt_vectorizer.vocabulary_, cnt_vec_sk), cnt_path) # save mapping as well for further analysis\n",
    "        if i == 0:\n",
    "            uid_file = \"uid.pkl\"\n",
    "            uid_path = os.path.join(cnt_dir, uid_file)\n",
    "            save_as_pickle(df['uid'].values, uid_path)  # save uid for further analysis\n",
    "        del cnt_vec_jz\n",
    "        del cnt_vec_sk\n",
    "        del cnt_dict\n",
    "        del cnt_vectorizer\n",
    "        gc.collect()\n",
    "\n",
    "    with profiler(\"TFIDF vectorizing (ngram=1) transformation\"):\n",
    "        tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab, \n",
    "                                           tokenizer=tokenizer,\n",
    "                                           dtype=np.float32)\n",
    "        tfidf_vec = tfidf_vectorizer.fit_transform(docs)\n",
    "        \n",
    "\n",
    "    with profiler(\"saving TFIDF vectors\", verbose_time=False):\n",
    "        tfidf_file = \"userFeature.[featureName='{}'].pkl\".format(feat_name)\n",
    "        tfidf_path = os.path.join(tfidf_dir, tfidf_file)\n",
    "        save_as_pickle((tfidf_vectorizer.vocabulary_, tfidf_vectorizer.idf_, tfidf_vec), tfidf_path)  # save mapping and idf as well\n",
    "        if i == 0:\n",
    "            uid_file = \"uid.pkl\"\n",
    "            uid_path = os.path.join(tfidf_dir, uid_file)\n",
    "            save_as_pickle(df['uid'].values, uid_path)  # save uid for further analysis\n",
    "        del tfidf_vec\n",
    "        del tfidf_vectorizer\n",
    "        gc.collect()\n",
    "\n",
    "    with profiler(\"cleaning\", verbose_time=False):\n",
    "        del [[docs, df]]\n",
    "        del vocab\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage at this moment: 1.18GB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(\"Memory usage at this moment: {}\".format(get_memory_str()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
