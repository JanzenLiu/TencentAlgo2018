{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import scipy.sparse as sparse\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../code/utils')\n",
    "sys.path.append('../code/pipeline')\n",
    "sys.path.append('../code')\n",
    "import data_utils as du\n",
    "import perf_utils as pu\n",
    "import data_jointer as dj\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhist_folder = os.path.join(config.DATA_DIR, \"click_history/simple_cross/byUserFeatureName\")\n",
    "clickrate_folder = os.path.join(config.DATA_DIR, \"clickrate_bs/simple_cross/byUserFeatureName\")\n",
    "\n",
    "\n",
    "def click_history_fold_dir(mode, num_folds):\n",
    "    folder = \"{}[{}_{}]\".format(clickhist_folder, mode, num_folds)\n",
    "    return folder\n",
    "\n",
    "\n",
    "def click_rate_fold_dir(mode, num_folds):\n",
    "    folder = \"{}[{}_{}]\".format(clickrate_folder, mode, num_folds)\n",
    "    return folder\n",
    "\n",
    "\n",
    "def click_rate_path(mode, num_folds, fold_index, ad_feat_name, user_feat_name):\n",
    "    folder = click_rate_fold_dir(mode, num_folds)\n",
    "    folder = os.path.join(folder, str(fold_index),  \"[featureName='{}']\".format(user_feat_name))\n",
    "    clickrate_file = \"[adFeatureName='{}'].csv\".format(ad_feat_name)\n",
    "    clickrate_filepath = os.path.join(folder, clickrate_file)\n",
    "    return clickrate_filepath\n",
    "\n",
    "\n",
    "def load_split_indices(mode, num_folds):\n",
    "    fold_dir = click_history_fold_dir(mode, num_folds=n_splits)\n",
    "    index_file = \"indices.pkl\"\n",
    "    index_path = os.path.join(fold_dir, index_file)\n",
    "    split_indices = du.load_pickle(index_path)\n",
    "    return split_indices\n",
    "\n",
    "\n",
    "def load_clickrate(mode, num_folds, fold_index, ad_feat_name, user_feat_name):\n",
    "    in_path = click_rate_path(mode, num_folds, fold_index, ad_feat_name, user_feat_name)\n",
    "    df_clickrate = pd.read_csv(in_path)\n",
    "    return df_clickrate\n",
    "\n",
    "\n",
    "def batch_load_clickrate(num_folds, ad_feat_name, user_feat_name):\n",
    "    quick_load = partial(load_clickrate, mode=\"StratifiedKFold\", num_folds=num_folds, \n",
    "                         ad_feat_name=ad_feat_name, user_feat_name=user_feat_name)\n",
    "    df_clickrate = None\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        df_new = quick_load(fold_index=i)\n",
    "        df_new[\"fold\"] = i\n",
    "        if df_clickrate is None:\n",
    "            df_clickrate = df_new\n",
    "        else:\n",
    "            df_clickrate = pd.concat([df_clickrate, df_new], ignore_index=True)\n",
    "        del df_new\n",
    "        gc.collect()\n",
    "        \n",
    "    df_clickrate[\"fold\"] = df_clickrate[\"fold\"].astype(int)\n",
    "    return df_clickrate\n",
    "\n",
    "\n",
    "def insert_valid_fold_index(df, fold_indices):\n",
    "    df = df.copy()\n",
    "    df[\"fold\"] = -1\n",
    "    for i, (train_index, valid_index) in enumerate(fold_indices):\n",
    "        df.loc[valid_index, \"fold\"] = i\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [# ('productId', 'LBS'),  # no need in this case\n",
    "         ('productType', 'kw1'),  # 'kw1' looks very overfitting prone, to be decide whether to keep it\n",
    "         ('productType', 'kw2'),\n",
    "         ('productType', 'kw3'),\n",
    "         ('productType', 'topic1'),\n",
    "         ('aid', 'topic2'),\n",
    "         ('productType', 'topic2'),\n",
    "         # ('productType', 'topic3'),  # might help in predicting negative samples\n",
    "         # ('productType', 'appIdInstall'),  # might help in predicting negative samples\n",
    "         # ('productType', 'appIdAction'),  # might help in predicting negative samples\n",
    "         ('advertiserId', 'interest1'),\n",
    "         ('aid', 'interest2'),\n",
    "         ('creativeSize', 'interest2'), \n",
    "         ('campaignId', 'interest4'),  # whether to keep it? \n",
    "         ('aid', 'interest5'),  \n",
    "         ('aid', 'ct'),\n",
    "         ('aid', 'os')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define jointer\n",
    "ad_jointer = dj.PandasPandasJointer(\"aid\")\n",
    "user_jointer = dj.PandasPandasJointer(\"uid\")\n",
    "\n",
    "# load DataFrame\n",
    "df_train = du.load_raw_data(\"train\")\n",
    "df_ad = du.load_raw_data(\"ad\")\n",
    "\n",
    "# load train/valid split indices\n",
    "mode = \"StratifiedKFold\"\n",
    "n_splits = 5\n",
    "split_indices = load_split_indices(mode, n_splits)\n",
    "\n",
    "# insert fold info\n",
    "df_train = insert_valid_fold_index(df_train, split_indices)\n",
    "\n",
    "# join ad features\n",
    "df_train = ad_jointer.join(df_train, df_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Processing 'productType' x 'kw1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:26:24] Finish constructing clickrate dictionary. △M: +998.73MB. △T: 6.5 minutes.\n",
      "[00:26:57] Finish splitting values. △M: +3.37GB. △T: 15.9 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:33<00:00, 264100.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:27:30] Finish joining click info. △M: +1.33GB. △T: 33.6 seconds.\n",
      "[00:27:50] Finish reseting DataFrame. △M: -3.31GB. △T: 20.0 seconds.\n",
      "[00:28:08] Finish saving intermediate result. △M: -268.45MB. △T: 17.5 seconds.\n",
      "Start Processing 'productType' x 'kw2'\n",
      "[00:29:32] Finish constructing clickrate dictionary. △M: -302.86MB. △T: 1.3 minutes.\n",
      "[00:30:04] Finish splitting values. △M: +3.07GB. △T: 18.1 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:27<00:00, 317814.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:30:34] Finish joining click info. △M: -123.39MB. △T: 29.4 seconds.\n",
      "[00:30:52] Finish reseting DataFrame. △M: -3.21GB. △T: 18.7 seconds.\n",
      "[00:31:07] Finish saving intermediate result. △M: -201.39MB. △T: 14.9 seconds.\n",
      "Start Processing 'productType' x 'kw3'\n",
      "[00:31:32] Finish constructing clickrate dictionary. △M: -4.32MB. △T: 18.1 seconds.\n",
      "[00:31:47] Finish splitting values. △M: +50.95MB. △T: 8.3 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:11<00:00, 736169.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:32:00] Finish joining click info. △M: +10.23MB. △T: 13.0 seconds.\n",
      "[00:32:10] Finish reseting DataFrame. △M: -655.42MB. △T: 9.5 seconds.\n",
      "[00:32:17] Finish saving intermediate result. △M: -201.39MB. △T: 7.1 seconds.\n",
      "Start Processing 'productType' x 'topic1'\n",
      "[00:32:36] Finish constructing clickrate dictionary. △M: +0B. △T: 15.0 seconds.\n",
      "[00:33:04] Finish splitting values. △M: +2.44GB. △T: 15.6 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:26<00:00, 326389.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:33:32] Finish joining click info. △M: +18.76MB. △T: 27.6 seconds.\n",
      "[00:33:50] Finish reseting DataFrame. △M: -2.96GB. △T: 18.6 seconds.\n",
      "[00:34:06] Finish saving intermediate result. △M: -201.39MB. △T: 16.1 seconds.\n",
      "Start Processing 'aid' x 'topic2'\n",
      "[00:45:02] Finish constructing clickrate dictionary. △M: +1.72GB. △T: 10.7 minutes.\n",
      "[00:45:33] Finish splitting values. △M: +2.39GB. △T: 18.1 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:44<00:00, 199623.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:46:18] Finish joining click info. △M: +275.89MB. △T: 45.2 seconds.\n",
      "[00:46:46] Finish reseting DataFrame. △M: -3.04GB. △T: 28.1 seconds.\n",
      "[00:47:11] Finish saving intermediate result. △M: -201.39MB. △T: 25.3 seconds.\n",
      "Start Processing 'productType' x 'topic2'\n",
      "[00:47:40] Finish constructing clickrate dictionary. △M: -713.0MB. △T: 17.4 seconds.\n",
      "[00:48:11] Finish splitting values. △M: +1.99GB. △T: 18.9 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:27<00:00, 323662.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:48:40] Finish joining click info. △M: -392.33MB. △T: 29.5 seconds.\n",
      "[00:49:00] Finish reseting DataFrame. △M: -2.58GB. △T: 19.3 seconds.\n",
      "[00:49:16] Finish saving intermediate result. △M: -201.39MB. △T: 16.3 seconds.\n",
      "Start Processing 'advertiserId' x 'interest1'\n",
      "[00:49:27] Finish constructing clickrate dictionary. △M: -9.25MB. △T: 3.6 seconds.\n",
      "[00:50:12] Finish splitting values. △M: +5.56GB. △T: 31.5 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:46<00:00, 189763.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:50:59] Finish joining click info. △M: +854.32MB. △T: 47.5 seconds.\n",
      "[00:51:38] Finish reseting DataFrame. △M: -6.04GB. △T: 38.4 seconds.\n",
      "[00:52:02] Finish saving intermediate result. △M: -201.39MB. △T: 24.5 seconds.\n",
      "Start Processing 'aid' x 'interest2'\n",
      "[00:52:17] Finish constructing clickrate dictionary. △M: +0B. △T: 5.1 seconds.\n",
      "[00:52:41] Finish splitting values. △M: +888.67MB. △T: 14.7 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:24<00:00, 353998.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:08] Finish joining click info. △M: +163.63MB. △T: 26.2 seconds.\n",
      "[00:53:27] Finish reseting DataFrame. △M: -1.62GB. △T: 18.9 seconds.\n",
      "[00:53:41] Finish saving intermediate result. △M: -201.39MB. △T: 14.1 seconds.\n",
      "Start Processing 'creativeSize' x 'interest2'\n",
      "[00:53:48] Finish constructing clickrate dictionary. △M: -768.0KB. △T: 0.5 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8798814 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:54:10] Finish splitting values. △M: +907.98MB. △T: 13.1 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:19<00:00, 451089.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:54:31] Finish joining click info. △M: -6.8MB. △T: 20.4 seconds.\n",
      "[00:54:47] Finish reseting DataFrame. △M: -1.61GB. △T: 16.4 seconds.\n",
      "[00:54:58] Finish saving intermediate result. △M: +0B. △T: 11.0 seconds.\n",
      "Start Processing 'campaignId' x 'interest4'\n",
      "[00:55:05] Finish constructing clickrate dictionary. △M: -256.0KB. △T: 0.5 seconds.\n",
      "[00:55:20] Finish splitting values. △M: -1.18MB. △T: 8.4 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:11<00:00, 756137.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:55:32] Finish joining click info. △M: -283.0MB. △T: 12.5 seconds.\n",
      "[00:55:41] Finish reseting DataFrame. △M: -738.43MB. △T: 9.3 seconds.\n",
      "[00:55:48] Finish saving intermediate result. △M: +0B. △T: 6.8 seconds.\n",
      "Start Processing 'aid' x 'interest5'\n",
      "[00:56:02] Finish constructing clickrate dictionary. △M: +0B. △T: 8.5 seconds.\n",
      "[00:56:49] Finish splitting values. △M: +6.44GB. △T: 34.6 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:59<00:00, 149059.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:57:48] Finish joining click info. △M: +729.44MB. △T: 59.8 seconds.\n",
      "[00:58:32] Finish reseting DataFrame. △M: -6.96GB. △T: 43.6 seconds.\n",
      "[00:59:04] Finish saving intermediate result. △M: -201.39MB. △T: 31.7 seconds.\n",
      "Start Processing 'aid' x 'ct'\n",
      "[00:59:16] Finish constructing clickrate dictionary. △M: +100.0KB. △T: 0.3 seconds.\n",
      "[00:59:36] Finish splitting values. △M: -1.0MB. △T: 11.8 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:14<00:00, 589905.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:59:52] Finish joining click info. △M: -300.63MB. △T: 16.2 seconds.\n",
      "[01:00:03] Finish reseting DataFrame. △M: -537.04MB. △T: 10.9 seconds.\n",
      "[01:00:11] Finish saving intermediate result. △M: -201.39MB. △T: 7.9 seconds.\n",
      "Start Processing 'aid' x 'os'\n",
      "[01:00:16] Finish constructing clickrate dictionary. △M: -4.0MB. △T: 0.2 seconds.\n",
      "[01:00:30] Finish splitting values. △M: -16.0MB. △T: 7.5 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8798814/8798814 [00:11<00:00, 755600.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:00:42] Finish joining click info. △M: -2.25MB. △T: 12.3 seconds.\n",
      "[01:00:52] Finish reseting DataFrame. △M: -738.43MB. △T: 9.3 seconds.\n",
      "[01:00:59] Finish saving intermediate result. △M: +0B. △T: 6.9 seconds.\n"
     ]
    }
   ],
   "source": [
    "for ad_feat_name, user_feat_name in pairs:\n",
    "    print(\"Start Processing '{}' x '{}'\".format(ad_feat_name, user_feat_name))\n",
    "    \n",
    "    # load clickrates\n",
    "    df_clickrate = batch_load_clickrate(n_splits, ad_feat_name, user_feat_name)\n",
    "\n",
    "    # reset type to save memory\n",
    "    df_clickrate = df_clickrate.astype({\n",
    "        \"bs_clickrate\": np.float32,\n",
    "        \"impression\": np.int32,\n",
    "        \"user_val\": str\n",
    "    })\n",
    "\n",
    "    # init clickrates dict\n",
    "    with pu.profiler(\"constructing clickrate dictionary\"):\n",
    "        ckr_dict = {(x[\"fold\"], x[\"ad_val\"], x[\"user_val\"]): (x[\"bs_clickrate\"], x[\"impression\"]) \n",
    "                    for i, x in df_clickrate.iterrows()}\n",
    "\n",
    "    # load user feature\n",
    "    df_user = du.load_user_feature(user_feat_name)\n",
    "\n",
    "    # join user feature\n",
    "    train = df_train.copy()\n",
    "    train = user_jointer.join(train, df_user)\n",
    "\n",
    "    # split list feature\n",
    "    with pu.profiler(\"splitting values\"):\n",
    "        train[\"val_list\"] = train[user_feat_name].fillna(\"[nan]\").apply(lambda x: x.split())\n",
    "\n",
    "    # join clickrates\n",
    "    with pu.profiler(\"joining click info\"):\n",
    "        ckrs = []\n",
    "        for (fold, aval, uvals) in tqdm.tqdm(zip(train[\"fold\"], train[ad_feat_name], train[\"val_list\"]), total=df_train.shape[0]):\n",
    "            row_ckrs = [ckr_dict[(fold, aval, uval)] for uval in uvals]\n",
    "            ckrs.append(row_ckrs)\n",
    "\n",
    "    # reset DataFrame\n",
    "    with pu.profiler(\"reseting DataFrame\"):\n",
    "        train = train[[\"aid\", \"uid\", \"fold\"]]\n",
    "        train.loc[:, \"click_list\"] = ckrs\n",
    "        gc.collect()\n",
    "\n",
    "    # save intermediate result\n",
    "    with pu.profiler(\"saving intermediate result\"):\n",
    "        agg_folder = os.path.join(config.DATA_DIR, \"intermediate/clickrate_aggregate\")\n",
    "        agg_file = \"train.[adFeatureName='{}'][userFeatureName='{}'].pkl\".format(ad_feat_name, user_feat_name)\n",
    "        agg_path = os.path.join(agg_folder, agg_file)\n",
    "        os.makedirs(agg_folder, exist_ok=True)\n",
    "        du.save_pickle(train, agg_path)\n",
    "        del train\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DataFrame\n",
    "df_test = du.load_raw_data(\"test\")\n",
    "\n",
    "# join ad features\n",
    "df_test = ad_jointer.join(df_test, df_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Processing 'productType' x 'kw1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:23:11] Finish constructing clickrate dictionary. △M: -2.75MB. △T: 6.1 minutes.\n",
      "[01:23:25] Finish splitting values. △M: +397.96MB. △T: 5.5 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:08<00:00, 262276.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:23:35] Finish joining click info. △M: -26.75MB. △T: 9.1 seconds.\n",
      "[01:23:41] Finish reseting DataFrame. △M: -398.75MB. △T: 6.1 seconds.\n",
      "[01:23:45] Finish saving intermediate result. △M: -512.0KB. △T: 4.6 seconds.\n",
      "Start Processing 'productType' x 'kw2'\n",
      "[01:25:00] Finish constructing clickrate dictionary. △M: -70.46MB. △T: 1.2 minutes.\n",
      "[01:25:14] Finish splitting values. △M: +221.84MB. △T: 5.2 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:07<00:00, 299290.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:25:22] Finish joining click info. △M: +0B. △T: 8.1 seconds.\n",
      "[01:25:27] Finish reseting DataFrame. △M: -221.75MB. △T: 5.4 seconds.\n",
      "[01:25:31] Finish saving intermediate result. △M: +0B. △T: 3.7 seconds.\n",
      "Start Processing 'productType' x 'kw3'\n",
      "[01:25:51] Finish constructing clickrate dictionary. △M: -1.25MB. △T: 17.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4194/2265989 [00:00<01:08, 33253.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:25:58] Finish splitting values. △M: -1.0MB. △T: 3.3 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:03<00:00, 622564.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:26:02] Finish joining click info. △M: +0B. △T: 4.0 seconds.\n",
      "[01:26:04] Finish reseting DataFrame. △M: -5.0MB. △T: 2.7 seconds.\n",
      "[01:26:06] Finish saving intermediate result. △M: +0B. △T: 1.9 seconds.\n",
      "Start Processing 'productType' x 'topic1'\n",
      "[01:26:23] Finish constructing clickrate dictionary. △M: -26.5MB. △T: 14.3 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2265989 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:26:34] Finish splitting values. △M: +60.78MB. △T: 4.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:07<00:00, 314706.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:26:42] Finish joining click info. △M: +0B. △T: 7.4 seconds.\n",
      "[01:26:47] Finish reseting DataFrame. △M: -63.25MB. △T: 5.0 seconds.\n",
      "[01:26:50] Finish saving intermediate result. △M: +0B. △T: 3.7 seconds.\n",
      "Start Processing 'aid' x 'topic2'\n",
      "[01:37:02] Finish constructing clickrate dictionary. △M: +309.17MB. △T: 10.1 minutes.\n",
      "[01:37:14] Finish splitting values. △M: +73.52MB. △T: 4.3 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:10<00:00, 208714.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:37:25] Finish joining click info. △M: +4.0MB. △T: 11.2 seconds.\n",
      "[01:37:33] Finish reseting DataFrame. △M: +0B. △T: 7.6 seconds.\n",
      "[01:37:39] Finish saving intermediate result. △M: -73.25MB. △T: 6.0 seconds.\n",
      "Start Processing 'productType' x 'topic2'\n",
      "[01:37:58] Finish constructing clickrate dictionary. △M: -141.75MB. △T: 14.8 seconds.\n",
      "[01:38:09] Finish splitting values. △M: -256.0KB. △T: 5.1 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:07<00:00, 322418.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:38:17] Finish joining click info. △M: -81.75MB. △T: 7.6 seconds.\n",
      "[01:38:22] Finish reseting DataFrame. △M: -10.75MB. △T: 4.7 seconds.\n",
      "[01:38:25] Finish saving intermediate result. △M: -512.0KB. △T: 3.5 seconds.\n",
      "Start Processing 'advertiserId' x 'interest1'\n",
      "[01:38:31] Finish constructing clickrate dictionary. △M: -6.25MB. △T: 3.3 seconds.\n",
      "[01:38:46] Finish splitting values. △M: +875.93MB. △T: 6.4 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:10<00:00, 213761.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:38:56] Finish joining click info. △M: +92.0KB. △T: 10.9 seconds.\n",
      "[01:39:05] Finish reseting DataFrame. △M: +0B. △T: 8.2 seconds.\n",
      "[01:39:10] Finish saving intermediate result. △M: -876.75MB. △T: 5.7 seconds.\n",
      "Start Processing 'aid' x 'interest2'\n",
      "[01:39:19] Finish constructing clickrate dictionary. △M: +0B. △T: 4.8 seconds.\n",
      "[01:39:28] Finish splitting values. △M: +0B. △T: 4.1 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:05<00:00, 396394.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:39:34] Finish joining click info. △M: +0B. △T: 6.0 seconds.\n",
      "[01:39:38] Finish reseting DataFrame. △M: +0B. △T: 4.1 seconds.\n",
      "[01:39:41] Finish saving intermediate result. △M: +0B. △T: 3.0 seconds.\n",
      "Start Processing 'creativeSize' x 'interest2'\n",
      "[01:39:44] Finish constructing clickrate dictionary. △M: -1.0MB. △T: 0.4 seconds.\n",
      "[01:39:52] Finish splitting values. △M: -256.0KB. △T: 3.7 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:05<00:00, 444518.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:39:57] Finish joining click info. △M: -256.0KB. △T: 5.3 seconds.\n",
      "[01:40:01] Finish reseting DataFrame. △M: +0B. △T: 4.0 seconds.\n",
      "[01:40:04] Finish saving intermediate result. △M: +0B. △T: 2.7 seconds.\n",
      "Start Processing 'campaignId' x 'interest4'\n",
      "[01:40:07] Finish constructing clickrate dictionary. △M: +0B. △T: 0.5 seconds.\n",
      "[01:40:13] Finish splitting values. △M: +0B. △T: 2.8 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:03<00:00, 650268.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:40:16] Finish joining click info. △M: +0B. △T: 3.7 seconds.\n",
      "[01:40:19] Finish reseting DataFrame. △M: +0B. △T: 2.4 seconds.\n",
      "[01:40:21] Finish saving intermediate result. △M: +0B. △T: 1.8 seconds.\n",
      "Start Processing 'aid' x 'interest5'\n",
      "[01:40:31] Finish constructing clickrate dictionary. △M: +0B. △T: 8.1 seconds.\n",
      "[01:40:46] Finish splitting values. △M: +1.14GB. △T: 6.7 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:13<00:00, 169165.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:40:59] Finish joining click info. △M: +892.0KB. △T: 13.6 seconds.\n",
      "[01:41:09] Finish reseting DataFrame. △M: -1.14GB. △T: 9.5 seconds.\n",
      "[01:41:16] Finish saving intermediate result. △M: +0B. △T: 6.6 seconds.\n",
      "Start Processing 'aid' x 'ct'\n",
      "[01:41:20] Finish constructing clickrate dictionary. △M: +0B. △T: 0.3 seconds.\n",
      "[01:41:27] Finish splitting values. △M: +0B. △T: 3.7 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:03<00:00, 578272.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:41:31] Finish joining click info. △M: -1.0MB. △T: 4.2 seconds.\n",
      "[01:41:34] Finish reseting DataFrame. △M: +0B. △T: 2.8 seconds.\n",
      "[01:41:36] Finish saving intermediate result. △M: +0B. △T: 2.0 seconds.\n",
      "Start Processing 'aid' x 'os'\n",
      "[01:41:38] Finish constructing clickrate dictionary. △M: +0B. △T: 0.2 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2265989 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:41:44] Finish splitting values. △M: +0B. △T: 2.6 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265989/2265989 [00:03<00:00, 653980.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:41:47] Finish joining click info. △M: +0B. △T: 3.7 seconds.\n",
      "[01:41:50] Finish reseting DataFrame. △M: +0B. △T: 2.4 seconds.\n",
      "[01:41:51] Finish saving intermediate result. △M: +0B. △T: 1.8 seconds.\n"
     ]
    }
   ],
   "source": [
    "for ad_feat_name, user_feat_name in pairs:\n",
    "    print(\"Start Processing '{}' x '{}'\".format(ad_feat_name, user_feat_name))\n",
    "    \n",
    "    # load clickrates\n",
    "    df_clickrate = batch_load_clickrate(n_splits, ad_feat_name, user_feat_name)\n",
    "\n",
    "    # reset type to save memory\n",
    "    df_clickrate = df_clickrate.astype({\n",
    "        \"bs_clickrate\": np.float32,\n",
    "        \"impression\": np.float32,\n",
    "        \"user_val\": str\n",
    "    })\n",
    "\n",
    "    # init clickrates dict\n",
    "    with pu.profiler(\"constructing clickrate dictionary\"):\n",
    "        raw_ckr_dict = {}\n",
    "        raw_imp_dict = {}\n",
    "        \n",
    "        for i, x in df_clickrate.iterrows():\n",
    "            tup = (x[\"ad_val\"], x[\"user_val\"])\n",
    "            if tup not in raw_ckr_dict:\n",
    "                raw_ckr_dict[tup] = x[\"bs_clickrate\"] / n_splits\n",
    "                raw_imp_dict[tup] = x[\"impression\"] / n_splits\n",
    "            else:\n",
    "                raw_ckr_dict[tup] += x[\"bs_clickrate\"] / n_splits\n",
    "                raw_imp_dict[tup] += x[\"impression\"] / n_splits\n",
    "                \n",
    "        ckr_dict = {tup: (raw_ckr_dict[tup], raw_imp_dict[tup]) for tup in raw_ckr_dict.keys()}\n",
    "        assert len(ckr_dict) == df_clickrate.shape[0] / n_splits\n",
    "\n",
    "    # load user feature\n",
    "    df_user = du.load_user_feature(user_feat_name)\n",
    "\n",
    "    # join user feature\n",
    "    test = df_test.copy()\n",
    "    test = user_jointer.join(test, df_user)\n",
    "\n",
    "    # split list feature\n",
    "    with pu.profiler(\"splitting values\"):\n",
    "        test[\"val_list\"] = test[user_feat_name].fillna(\"[nan]\").apply(lambda x: x.split())\n",
    "\n",
    "    # join clickrates\n",
    "    with pu.profiler(\"joining click info\"):\n",
    "        ckrs = []\n",
    "        for (aval, uvals) in tqdm.tqdm(zip(test[ad_feat_name], test[\"val_list\"]), total=df_test.shape[0]):\n",
    "            row_ckrs = [ckr_dict[(aval, uval)] for uval in uvals]\n",
    "            ckrs.append(row_ckrs)\n",
    "\n",
    "    # reset DataFrame\n",
    "    with pu.profiler(\"reseting DataFrame\"):\n",
    "        test = test[[\"aid\", \"uid\"]]\n",
    "        test.loc[:, \"click_list\"] = ckrs\n",
    "        gc.collect()\n",
    "\n",
    "    # save intermediate result\n",
    "    with pu.profiler(\"saving intermediate result\"):\n",
    "        agg_folder = os.path.join(config.DATA_DIR, \"intermediate/clickrate_aggregate\")\n",
    "        agg_file = \"test1.[adFeatureName='{}'][userFeatureName='{}'].pkl\".format(ad_feat_name, user_feat_name)\n",
    "        agg_path = os.path.join(agg_folder, agg_file)\n",
    "        os.makedirs(agg_folder, exist_ok=True)\n",
    "        du.save_pickle(test, agg_path)\n",
    "        del test\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# quantile of click rates\n",
    "# =======================\n",
    "# 5 features\n",
    "def get_ckr_quantiles(lst):\n",
    "    ckrs = [tup[0] for tup in lst]\n",
    "    return {q: np.percentile(ckrs, q) for q in [100, 75, 50, 25, 0]}\n",
    "\n",
    "\n",
    "def batch_get_ckr_quantiles(series):\n",
    "    quantiles = []\n",
    "    for i, x in series.iteritems():\n",
    "        quantiles.append(get_ckr_quantiles(x))\n",
    "    # print(\"[{}] batch done.\".format(pu.get_time_str()))\n",
    "    return quantiles\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# mean and std of click rates\n",
    "# ===========================\n",
    "# 2 features\n",
    "def get_ckr_stats(lst):\n",
    "    ckrs = [tup[0] for tup in lst]\n",
    "    return {\"mean\": np.mean(ckrs), \"std\": np.std(ckrs)}\n",
    "\n",
    "\n",
    "def batch_get_ckr_stats(series):\n",
    "    stats = []\n",
    "    for i, x in series.iteritems():\n",
    "        stats.append(get_ckr_stats(x))\n",
    "    # print(\"[{}] batch done.\".format(pu.get_time_str()))\n",
    "    return stats\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# mean and std of impressions\n",
    "# ===========================\n",
    "# 2 features\n",
    "def get_imp_stats(lst):\n",
    "    imps = [tup[1] for tup in lst]\n",
    "    return {\"mean\": np.mean(imps), \"std\": np.std(imps)}\n",
    "\n",
    "\n",
    "def batch_get_imp_stats(series):\n",
    "    stats = []\n",
    "    for i, x in series.iteritems():\n",
    "        stats.append(get_imp_stats(x))\n",
    "    # print(\"[{}] batch done.\".format(pu.get_time_str()))\n",
    "    return stats\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# impressions corresponding to min and max click rates\n",
    "# ====================================================\n",
    "# 2 features\n",
    "def get_corr_stats(lst):\n",
    "    tups = sorted(lst, key=lambda x: x[0])\n",
    "    return {\"min\": tups[0][1], \"max\": tups[-1][1]}\n",
    "\n",
    "\n",
    "def batch_get_corr_stats(series):\n",
    "    stats = []\n",
    "    for i, x in series.iteritems():\n",
    "        stats.append(get_corr_stats(x))\n",
    "    # print(\"[{}] batch done.\".format(pu.get_time_str()))\n",
    "    return stats\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# weighted average of click rates\n",
    "# ===============================\n",
    "def get_ckr_weighted_avg(lst):\n",
    "    ckrs = [tup[0] for tup in lst]\n",
    "    logimps = [np.log1p(tup[1]) for tup in lst]\n",
    "    try:\n",
    "        weighted_avg = np.average(ckrs, weights=logimps)\n",
    "    except ZeroDivisionError:\n",
    "        weighted_avg = np.mean(ckrs)\n",
    "    return {\"avg\": weighted_avg}\n",
    "\n",
    "\n",
    "def batch_get_ckr_weighted_avg(series):\n",
    "    stats = []\n",
    "    for i, x in series.iteritems():\n",
    "        stats.append(get_ckr_weighted_avg(x))\n",
    "    # print(\"[{}] batch done.\".format(pu.get_time_str()))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_pair_train(ad_feat_name, user_feat_name, batch_size=100000, n_procs=4):\n",
    "    print(\"Start processing '{}' x '{}'\".format(ad_feat_name, user_feat_name))\n",
    "    \n",
    "    # ========================\n",
    "    # load intermediate result\n",
    "    # ========================\n",
    "    agg_folder = os.path.join(config.DATA_DIR, \"intermediate/clickrate_aggregate\")\n",
    "    agg_file = \"train.[adFeatureName='{}'][userFeatureName='{}'].pkl\".format(ad_feat_name, user_feat_name)\n",
    "    agg_path = os.path.join(agg_folder, agg_file)\n",
    "    df_agg = du.load_pickle(agg_path)\n",
    "\n",
    "    # =============\n",
    "    # split batches\n",
    "    # =============\n",
    "    with pu.profiler(\"splitting batches\"):\n",
    "        indices = [(offset, offset + batch_size) for offset in range(0, df_agg.shape[0], batch_size)]\n",
    "        batches = [df_agg.iloc[tup[0]:tup[1]][\"click_list\"] for tup in indices]\n",
    "    print(\"{} batches, each with {} rows\".format(len(batches), batch_size))\n",
    "\n",
    "    # ====================\n",
    "    # click rate quantiles\n",
    "    # ====================\n",
    "    # process batches \n",
    "    with pu.profiler(\"extracting quantile features\"):\n",
    "        pool = mp.Pool(processes=n_procs)\n",
    "        results = [pool.apply_async(batch_get_ckr_quantiles, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        # merge batch results\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_ckr_quantile = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        # reset DataFrame\n",
    "        colname_dict = {q: \"bsClickrate@{}_x_{}_q{}\".format(ad_feat_name, user_feat_name, q) for q in [100, 75, 50, 25, 0]}\n",
    "        df_ckr_quantile = df_ckr_quantile.rename(columns=colname_dict)\n",
    "        df_ckr_quantile = df_ckr_quantile.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # =======================\n",
    "    # click rate mean and std\n",
    "    # =======================\n",
    "    with pu.profiler(\"extracting statistical features\"):\n",
    "        pool = mp.Pool(processes=8)\n",
    "        results = [pool.apply_async(batch_get_ckr_stats, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_ckr_stats = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        colname_dict = {s: \"bsClickrate@{}_x_{}_{}\".format(ad_feat_name, user_feat_name, s) for s in [\"mean\", \"std\"]}\n",
    "        df_ckr_stats = df_ckr_stats.rename(columns=colname_dict)\n",
    "        df_ckr_stats = df_ckr_stats.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # =======================\n",
    "    # impression mean and std\n",
    "    # =======================\n",
    "    with pu.profiler(\"extracting statistical features\"):\n",
    "        pool = mp.Pool(processes=8)\n",
    "        results = [pool.apply_async(batch_get_imp_stats, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_imp_stats = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        colname_dict = {s: \"impression@{}_x_{}_{}\".format(ad_feat_name, user_feat_name, s) for s in [\"mean\", \"std\"]}\n",
    "        df_imp_stats = df_imp_stats.rename(columns=colname_dict)\n",
    "        df_imp_stats = df_imp_stats.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # =======================================\n",
    "    # impressions for min and max click rates\n",
    "    # =======================================\n",
    "    with pu.profiler(\"extracting correlated statistical features\"):\n",
    "        pool = mp.Pool(processes=8)\n",
    "        results = [pool.apply_async(batch_get_corr_stats, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_corr_stats = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        colname_dict = {s: \"{}_bsClickrate_impression@{}_x_{}\".format(s, ad_feat_name, user_feat_name) for s in [\"min\", \"max\"]}\n",
    "        df_corr_stats = df_corr_stats.rename(columns=colname_dict)\n",
    "        df_corr_stats = df_corr_stats.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # ===========================\n",
    "    # click rate wegihted average\n",
    "    # ===========================\n",
    "    with pu.profiler(\"extracting weighted average features\"):\n",
    "        pool = mp.Pool(processes=8)\n",
    "        results = [pool.apply_async(batch_get_ckr_weighted_avg, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_ckr_wavg = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        colname_dict = {\"avg\": \"bsClickrate_weighted_avg@{}_x_{}\".format(ad_feat_name, user_feat_name)}\n",
    "        df_ckr_wavg = df_ckr_wavg.rename(columns=colname_dict)\n",
    "        df_ckr_wavg = df_ckr_wavg.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # ===================\n",
    "    # merge all DataFrame\n",
    "    # ===================\n",
    "    with pu.profiler(\"merging all features\"):\n",
    "        df_stack = pd.concat([df_ckr_quantile, df_ckr_wavg, df_ckr_stats, df_imp_stats, df_corr_stats], axis=1)\n",
    "        assert df_stack.isnull().sum().sum() == 0\n",
    "        assert df_stack.shape[0] == df_train.shape[0]\n",
    "        assert df_stack.shape[1] == 12\n",
    "        del df_ckr_quantile\n",
    "        del df_ckr_wavg\n",
    "        del df_ckr_stats\n",
    "        del df_imp_stats\n",
    "        del df_corr_stats\n",
    "        gc.collect()\n",
    "\n",
    "    with pu.profiler(\"saving features\"):\n",
    "        stack_folder = os.path.join(config.DATA_DIR, \"stacking/clickrate\")\n",
    "        stack_file = \"train.[adFeatureName='{}'][userFeatureName='{}'].pkl\".format(ad_feat_name, user_feat_name)\n",
    "        stack_path = os.path.join(stack_folder, stack_file)\n",
    "        os.makedirs(stack_folder, exist_ok=True)\n",
    "        du.save_pickle(df_stack, stack_path)\n",
    "        del df_stack\n",
    "        del result_list\n",
    "        del final_list\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing 'productType' x 'kw1'\n",
      "[04:28:58] Finish splitting batches. △M: +16.0KB. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[04:41:40] Finish extracting quantile features. △M: +2.96GB. △T: 12.7 minutes.\n",
      "[04:43:18] Finish extracting statistical features. △M: +2.66GB. △T: 1.6 minutes.\n",
      "[04:45:06] Finish extracting statistical features. △M: +1.28GB. △T: 1.8 minutes.\n",
      "[04:45:43] Finish extracting correlated statistical features. △M: -1.74GB. △T: 36.8 seconds.\n",
      "[04:46:55] Finish extracting weighted average features. △M: +1.75GB. △T: 1.2 minutes.\n",
      "[04:46:59] Finish merging all features. △M: +402.55MB. △T: 3.8 seconds.\n",
      "[04:47:01] Finish saving features. △M: -402.78MB. △T: 2.3 seconds.\n",
      "Start processing 'productType' x 'kw2'\n",
      "[04:47:29] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[05:00:19] Finish extracting quantile features. △M: +858.2MB. △T: 12.8 minutes.\n",
      "[05:02:01] Finish extracting statistical features. △M: +628.05MB. △T: 1.7 minutes.\n",
      "[05:03:50] Finish extracting statistical features. △M: -654.16MB. △T: 1.8 minutes.\n",
      "[05:04:27] Finish extracting correlated statistical features. △M: +2.38GB. △T: 37.2 seconds.\n",
      "[05:05:43] Finish extracting weighted average features. △M: -2.42GB. △T: 1.3 minutes.\n",
      "[05:05:47] Finish merging all features. △M: +402.28MB. △T: 4.0 seconds.\n",
      "[05:05:49] Finish saving features. △M: -402.78MB. △T: 2.4 seconds.\n",
      "Start processing 'productType' x 'kw3'\n",
      "[05:06:06] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[05:19:02] Finish extracting quantile features. △M: +2.84GB. △T: 12.9 minutes.\n",
      "[05:20:38] Finish extracting statistical features. △M: +697.85MB. △T: 1.6 minutes.\n",
      "[05:22:21] Finish extracting statistical features. △M: -707.95MB. △T: 1.7 minutes.\n",
      "[05:22:52] Finish extracting correlated statistical features. △M: +406.14MB. △T: 31.0 seconds.\n",
      "[05:23:55] Finish extracting weighted average features. △M: -417.75MB. △T: 1.1 minutes.\n",
      "[05:23:59] Finish merging all features. △M: +402.53MB. △T: 3.2 seconds.\n",
      "[05:24:00] Finish saving features. △M: -402.78MB. △T: 1.8 seconds.\n",
      "Start processing 'productType' x 'topic1'\n",
      "[05:24:27] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[05:37:21] Finish extracting quantile features. △M: -1.21GB. △T: 12.9 minutes.\n",
      "[05:39:02] Finish extracting statistical features. △M: +2.69GB. △T: 1.7 minutes.\n",
      "[05:40:51] Finish extracting statistical features. △M: -2.76GB. △T: 1.8 minutes.\n",
      "[05:41:26] Finish extracting correlated statistical features. △M: +400.91MB. △T: 34.9 seconds.\n",
      "[05:42:41] Finish extracting weighted average features. △M: +1.59GB. △T: 1.2 minutes.\n",
      "[05:42:45] Finish merging all features. △M: +402.78MB. △T: 4.0 seconds.\n",
      "[05:42:47] Finish saving features. △M: -402.78MB. △T: 2.4 seconds.\n",
      "Start processing 'aid' x 'topic2'\n",
      "[05:43:32] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[05:56:28] Finish extracting quantile features. △M: +762.39MB. △T: 12.9 minutes.\n",
      "[05:58:13] Finish extracting statistical features. △M: +839.52MB. △T: 1.7 minutes.\n",
      "[06:00:06] Finish extracting statistical features. △M: +1.27GB. △T: 1.9 minutes.\n",
      "[06:00:48] Finish extracting correlated statistical features. △M: -1.98GB. △T: 42.0 seconds.\n",
      "[06:02:06] Finish extracting weighted average features. △M: +1.91GB. △T: 1.3 minutes.\n",
      "[06:02:10] Finish merging all features. △M: +402.78MB. △T: 4.5 seconds.\n",
      "[06:02:13] Finish saving features. △M: -402.78MB. △T: 3.0 seconds.\n",
      "Start processing 'productType' x 'topic2'\n",
      "[06:02:44] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[06:15:31] Finish extracting quantile features. △M: +800.95MB. △T: 12.8 minutes.\n",
      "[06:17:11] Finish extracting statistical features. △M: +2.75GB. △T: 1.7 minutes.\n",
      "[06:19:02] Finish extracting statistical features. △M: -2.68GB. △T: 1.8 minutes.\n",
      "[06:19:36] Finish extracting correlated statistical features. △M: +401.76MB. △T: 33.7 seconds.\n",
      "[06:20:49] Finish extracting weighted average features. △M: -388.69MB. △T: 1.2 minutes.\n",
      "[06:20:53] Finish merging all features. △M: +402.28MB. △T: 3.7 seconds.\n",
      "[06:20:55] Finish saving features. △M: -402.78MB. △T: 2.2 seconds.\n",
      "Start processing 'advertiserId' x 'interest1'\n",
      "[06:21:37] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[06:34:36] Finish extracting quantile features. △M: +2.81GB. △T: 13.0 minutes.\n",
      "[06:36:29] Finish extracting statistical features. △M: -1.3GB. △T: 1.9 minutes.\n",
      "[06:38:29] Finish extracting statistical features. △M: +1.21GB. △T: 2.0 minutes.\n",
      "[06:39:11] Finish extracting correlated statistical features. △M: -1.5GB. △T: 42.0 seconds.\n",
      "[06:40:49] Finish extracting weighted average features. △M: -463.81MB. △T: 1.6 minutes.\n",
      "[06:40:53] Finish merging all features. △M: +335.4MB. △T: 4.8 seconds.\n",
      "[06:40:56] Finish saving features. △M: -402.78MB. △T: 2.7 seconds.\n",
      "Start processing 'aid' x 'interest2'\n",
      "[06:41:23] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[06:54:11] Finish extracting quantile features. △M: +886.65MB. △T: 12.8 minutes.\n",
      "[06:55:51] Finish extracting statistical features. △M: +2.68GB. △T: 1.7 minutes.\n",
      "[06:57:38] Finish extracting statistical features. △M: -2.73GB. △T: 1.8 minutes.\n",
      "[06:58:10] Finish extracting correlated statistical features. △M: +399.57MB. △T: 32.0 seconds.\n",
      "[06:59:21] Finish extracting weighted average features. △M: +1.63GB. △T: 1.2 minutes.\n",
      "[06:59:24] Finish merging all features. △M: +402.78MB. △T: 3.6 seconds.\n",
      "[06:59:26] Finish saving features. △M: -402.78MB. △T: 2.1 seconds.\n",
      "Start processing 'creativeSize' x 'interest2'\n",
      "[06:59:51] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[07:12:41] Finish extracting quantile features. △M: +805.6MB. △T: 12.8 minutes.\n",
      "[07:14:19] Finish extracting statistical features. △M: +728.58MB. △T: 1.6 minutes.\n",
      "[07:16:04] Finish extracting statistical features. △M: +1.32GB. △T: 1.8 minutes.\n",
      "[07:16:37] Finish extracting correlated statistical features. △M: -1.63GB. △T: 32.4 seconds.\n",
      "[07:17:46] Finish extracting weighted average features. △M: -489.34MB. △T: 1.2 minutes.\n",
      "[07:17:49] Finish merging all features. △M: +402.53MB. △T: 3.6 seconds.\n",
      "[07:17:51] Finish saving features. △M: -402.78MB. △T: 2.0 seconds.\n",
      "Start processing 'campaignId' x 'interest4'\n",
      "[07:18:07] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[07:30:54] Finish extracting quantile features. △M: +2.83GB. △T: 12.8 minutes.\n",
      "[07:32:29] Finish extracting statistical features. △M: +701.46MB. △T: 1.6 minutes.\n",
      "[07:34:10] Finish extracting statistical features. △M: -646.21MB. △T: 1.7 minutes.\n",
      "[07:34:40] Finish extracting correlated statistical features. △M: -1.59GB. △T: 30.4 seconds.\n",
      "[07:35:40] Finish extracting weighted average features. △M: +1.57GB. △T: 1.0 minutes.\n",
      "[07:35:44] Finish merging all features. △M: +402.53MB. △T: 3.1 seconds.\n",
      "[07:35:45] Finish saving features. △M: -402.78MB. △T: 1.7 seconds.\n",
      "Start processing 'aid' x 'interest5'\n",
      "[07:36:34] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[07:49:41] Finish extracting quantile features. △M: +823.93MB. △T: 13.1 minutes.\n",
      "[07:51:36] Finish extracting statistical features. △M: +739.46MB. △T: 1.9 minutes.\n",
      "[07:53:38] Finish extracting statistical features. △M: -749.88MB. △T: 2.0 minutes.\n",
      "[07:54:22] Finish extracting correlated statistical features. △M: +2.42GB. △T: 44.6 seconds.\n",
      "[07:56:09] Finish extracting weighted average features. △M: -2.45GB. △T: 1.8 minutes.\n",
      "[07:56:14] Finish merging all features. △M: +402.28MB. △T: 4.6 seconds.\n",
      "[07:56:17] Finish saving features. △M: -402.78MB. △T: 3.0 seconds.\n",
      "Start processing 'aid' x 'ct'\n",
      "[07:56:37] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[08:09:24] Finish extracting quantile features. △M: +801.5MB. △T: 12.8 minutes.\n",
      "[08:11:00] Finish extracting statistical features. △M: +723.68MB. △T: 1.6 minutes.\n",
      "[08:12:44] Finish extracting statistical features. △M: +1.32GB. △T: 1.7 minutes.\n",
      "[08:13:15] Finish extracting correlated statistical features. △M: -1.63GB. △T: 31.1 seconds.\n",
      "[08:14:27] Finish extracting weighted average features. △M: +1.56GB. △T: 1.2 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:14:31] Finish merging all features. △M: +402.78MB. △T: 3.5 seconds.\n",
      "[08:14:33] Finish saving features. △M: -402.78MB. △T: 1.9 seconds.\n",
      "Start processing 'aid' x 'os'\n",
      "[08:14:51] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "88 batches, each with 100000 rows\n",
      "[08:27:50] Finish extracting quantile features. △M: +795.06MB. △T: 13.0 minutes.\n",
      "[08:29:25] Finish extracting statistical features. △M: +807.92MB. △T: 1.6 minutes.\n",
      "[08:31:11] Finish extracting statistical features. △M: +1.3GB. △T: 1.8 minutes.\n",
      "[08:31:42] Finish extracting correlated statistical features. △M: -1.63GB. △T: 30.5 seconds.\n",
      "[08:32:43] Finish extracting weighted average features. △M: -427.01MB. △T: 1.0 minutes.\n",
      "[08:32:46] Finish merging all features. △M: +402.53MB. △T: 3.3 seconds.\n",
      "[08:32:48] Finish saving features. △M: -402.78MB. △T: 1.8 seconds.\n"
     ]
    }
   ],
   "source": [
    "for ad_feat_name, user_feat_name in pairs:\n",
    "    process_pair_train(ad_feat_name, user_feat_name, n_procs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pair_test(ad_feat_name, user_feat_name, batch_size=100000, n_procs=4):\n",
    "    print(\"Start processing '{}' x '{}'\".format(ad_feat_name, user_feat_name))\n",
    "    \n",
    "    # ========================\n",
    "    # load intermediate result\n",
    "    # ========================\n",
    "    agg_folder = os.path.join(config.DATA_DIR, \"intermediate/clickrate_aggregate\")\n",
    "    agg_file = \"test1.[adFeatureName='{}'][userFeatureName='{}'].pkl\".format(ad_feat_name, user_feat_name)\n",
    "    agg_path = os.path.join(agg_folder, agg_file)\n",
    "    df_agg = du.load_pickle(agg_path)\n",
    "\n",
    "    # =============\n",
    "    # split batches\n",
    "    # =============\n",
    "    with pu.profiler(\"splitting batches\"):\n",
    "        indices = [(offset, offset + batch_size) for offset in range(0, df_agg.shape[0], batch_size)]\n",
    "        batches = [df_agg.iloc[tup[0]:tup[1]][\"click_list\"] for tup in indices]\n",
    "    print(\"{} batches, each with {} rows\".format(len(batches), batch_size))\n",
    "\n",
    "    # ====================\n",
    "    # click rate quantiles\n",
    "    # ====================\n",
    "    # process batches \n",
    "    with pu.profiler(\"extracting quantile features\"):\n",
    "        pool = mp.Pool(processes=n_procs)\n",
    "        results = [pool.apply_async(batch_get_ckr_quantiles, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        # merge batch results\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_ckr_quantile = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        # reset DataFrame\n",
    "        colname_dict = {q: \"bsClickrate@{}_x_{}_q{}\".format(ad_feat_name, user_feat_name, q) for q in [100, 75, 50, 25, 0]}\n",
    "        df_ckr_quantile = df_ckr_quantile.rename(columns=colname_dict)\n",
    "        df_ckr_quantile = df_ckr_quantile.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # =======================\n",
    "    # click rate mean and std\n",
    "    # =======================\n",
    "    with pu.profiler(\"extracting statistical features\"):\n",
    "        pool = mp.Pool(processes=8)\n",
    "        results = [pool.apply_async(batch_get_ckr_stats, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_ckr_stats = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        colname_dict = {s: \"bsClickrate@{}_x_{}_{}\".format(ad_feat_name, user_feat_name, s) for s in [\"mean\", \"std\"]}\n",
    "        df_ckr_stats = df_ckr_stats.rename(columns=colname_dict)\n",
    "        df_ckr_stats = df_ckr_stats.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # =======================\n",
    "    # impression mean and std\n",
    "    # =======================\n",
    "    with pu.profiler(\"extracting statistical features\"):\n",
    "        pool = mp.Pool(processes=8)\n",
    "        results = [pool.apply_async(batch_get_imp_stats, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_imp_stats = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        colname_dict = {s: \"impression@{}_x_{}_{}\".format(ad_feat_name, user_feat_name, s) for s in [\"mean\", \"std\"]}\n",
    "        df_imp_stats = df_imp_stats.rename(columns=colname_dict)\n",
    "        df_imp_stats = df_imp_stats.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # =======================================\n",
    "    # impressions for min and max click rates\n",
    "    # =======================================\n",
    "    with pu.profiler(\"extracting correlated statistical features\"):\n",
    "        pool = mp.Pool(processes=8)\n",
    "        results = [pool.apply_async(batch_get_corr_stats, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_corr_stats = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        colname_dict = {s: \"{}_bsClickrate_impression@{}_x_{}\".format(s, ad_feat_name, user_feat_name) for s in [\"min\", \"max\"]}\n",
    "        df_corr_stats = df_corr_stats.rename(columns=colname_dict)\n",
    "        df_corr_stats = df_corr_stats.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # ===========================\n",
    "    # click rate wegihted average\n",
    "    # ===========================\n",
    "    with pu.profiler(\"extracting weighted average features\"):\n",
    "        pool = mp.Pool(processes=8)\n",
    "        results = [pool.apply_async(batch_get_ckr_weighted_avg, (batch, )) for batch in batches]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_list = [result.get() for result in results]\n",
    "\n",
    "        final_list = []\n",
    "        for result in result_list:\n",
    "            final_list += result\n",
    "        df_ckr_wavg = pd.DataFrame.from_dict(final_list)\n",
    "\n",
    "        colname_dict = {\"avg\": \"bsClickrate_weighted_avg@{}_x_{}\".format(ad_feat_name, user_feat_name)}\n",
    "        df_ckr_wavg = df_ckr_wavg.rename(columns=colname_dict)\n",
    "        df_ckr_wavg = df_ckr_wavg.astype({colname: np.float32 for colname in colname_dict.values()})\n",
    "        gc.collect()\n",
    "\n",
    "    # ===================\n",
    "    # merge all DataFrame\n",
    "    # ===================\n",
    "    with pu.profiler(\"merging all features\"):\n",
    "        df_stack = pd.concat([df_ckr_quantile, df_ckr_wavg, df_ckr_stats, df_imp_stats, df_corr_stats], axis=1)\n",
    "        assert df_stack.isnull().sum().sum() == 0\n",
    "        assert df_stack.shape[0] == df_test.shape[0]\n",
    "        assert df_stack.shape[1] == 12\n",
    "        del df_ckr_quantile\n",
    "        del df_ckr_wavg\n",
    "        del df_ckr_stats\n",
    "        del df_imp_stats\n",
    "        del df_corr_stats\n",
    "        gc.collect()\n",
    "\n",
    "    with pu.profiler(\"saving features\"):\n",
    "        stack_folder = os.path.join(config.DATA_DIR, \"stacking/clickrate\")\n",
    "        stack_file = \"test1.[adFeatureName='{}'][userFeatureName='{}'].pkl\".format(ad_feat_name, user_feat_name)\n",
    "        stack_path = os.path.join(stack_folder, stack_file)\n",
    "        os.makedirs(stack_folder, exist_ok=True)\n",
    "        du.save_pickle(df_stack, stack_path)\n",
    "        del df_stack\n",
    "        del result_list\n",
    "        del final_list\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing 'productType' x 'kw1'\n",
      "[08:39:51] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[08:43:25] Finish extracting quantile features. △M: +4.0KB. △T: 3.6 minutes.\n",
      "[08:44:01] Finish extracting statistical features. △M: +0B. △T: 35.5 seconds.\n",
      "[08:44:36] Finish extracting statistical features. △M: +4.0KB. △T: 35.6 seconds.\n",
      "[08:44:53] Finish extracting correlated statistical features. △M: +0B. △T: 16.6 seconds.\n",
      "[08:45:20] Finish extracting weighted average features. △M: +0B. △T: 27.4 seconds.\n",
      "[08:45:22] Finish merging all features. △M: +0B. △T: 1.8 seconds.\n",
      "[08:45:24] Finish saving features. △M: +0B. △T: 1.5 seconds.\n",
      "Start processing 'productType' x 'kw2'\n",
      "[08:45:32] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[08:49:04] Finish extracting quantile features. △M: +4.4MB. △T: 3.5 minutes.\n",
      "[08:49:39] Finish extracting statistical features. △M: +0B. △T: 35.4 seconds.\n",
      "[08:50:15] Finish extracting statistical features. △M: +0B. △T: 36.3 seconds.\n",
      "[08:50:32] Finish extracting correlated statistical features. △M: +0B. △T: 16.5 seconds.\n",
      "[08:51:00] Finish extracting weighted average features. △M: +0B. △T: 28.1 seconds.\n",
      "[08:51:02] Finish merging all features. △M: +0B. △T: 2.0 seconds.\n",
      "[08:51:04] Finish saving features. △M: +0B. △T: 1.6 seconds.\n",
      "Start processing 'productType' x 'kw3'\n",
      "[08:51:09] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[08:54:49] Finish extracting quantile features. △M: +4.0KB. △T: 3.7 minutes.\n",
      "[08:55:23] Finish extracting statistical features. △M: +0B. △T: 33.5 seconds.\n",
      "[08:55:58] Finish extracting statistical features. △M: +0B. △T: 34.6 seconds.\n",
      "[08:56:12] Finish extracting correlated statistical features. △M: +0B. △T: 14.8 seconds.\n",
      "[08:56:37] Finish extracting weighted average features. △M: +0B. △T: 24.4 seconds.\n",
      "[08:56:39] Finish merging all features. △M: +0B. △T: 1.7 seconds.\n",
      "[08:56:40] Finish saving features. △M: +0B. △T: 1.3 seconds.\n",
      "Start processing 'productType' x 'topic1'\n",
      "[08:56:48] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:00:23] Finish extracting quantile features. △M: +4.0KB. △T: 3.6 minutes.\n",
      "[09:00:59] Finish extracting statistical features. △M: +0B. △T: 35.5 seconds.\n",
      "[09:01:35] Finish extracting statistical features. △M: +0B. △T: 35.6 seconds.\n",
      "[09:01:51] Finish extracting correlated statistical features. △M: +0B. △T: 16.1 seconds.\n",
      "[09:02:18] Finish extracting weighted average features. △M: +0B. △T: 27.3 seconds.\n",
      "[09:02:20] Finish merging all features. △M: +0B. △T: 1.8 seconds.\n",
      "[09:02:21] Finish saving features. △M: +0B. △T: 1.5 seconds.\n",
      "Start processing 'aid' x 'topic2'\n",
      "[09:02:34] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:06:09] Finish extracting quantile features. △M: +4.0KB. △T: 3.6 minutes.\n",
      "[09:06:46] Finish extracting statistical features. △M: +0B. △T: 36.2 seconds.\n",
      "[09:07:22] Finish extracting statistical features. △M: +0B. △T: 36.3 seconds.\n",
      "[09:07:40] Finish extracting correlated statistical features. △M: +0B. △T: 17.9 seconds.\n",
      "[09:08:08] Finish extracting weighted average features. △M: +0B. △T: 28.1 seconds.\n",
      "[09:08:10] Finish merging all features. △M: +0B. △T: 2.1 seconds.\n",
      "[09:08:12] Finish saving features. △M: +0B. △T: 1.9 seconds.\n",
      "Start processing 'productType' x 'topic2'\n",
      "[09:08:21] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:11:53] Finish extracting quantile features. △M: +4.0KB. △T: 3.5 minutes.\n",
      "[09:12:28] Finish extracting statistical features. △M: +0B. △T: 35.3 seconds.\n",
      "[09:13:04] Finish extracting statistical features. △M: +0B. △T: 35.7 seconds.\n",
      "[09:13:21] Finish extracting correlated statistical features. △M: +0B. △T: 17.3 seconds.\n",
      "[09:13:50] Finish extracting weighted average features. △M: +0B. △T: 29.0 seconds.\n",
      "[09:13:52] Finish merging all features. △M: +0B. △T: 1.9 seconds.\n",
      "[09:13:54] Finish saving features. △M: +0B. △T: 1.5 seconds.\n",
      "Start processing 'advertiserId' x 'interest1'\n",
      "[09:14:07] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:18:18] Finish extracting quantile features. △M: +4.0KB. △T: 4.2 minutes.\n",
      "[09:18:55] Finish extracting statistical features. △M: +0B. △T: 37.0 seconds.\n",
      "[09:19:32] Finish extracting statistical features. △M: +0B. △T: 37.0 seconds.\n",
      "[09:19:50] Finish extracting correlated statistical features. △M: +0B. △T: 17.4 seconds.\n",
      "[09:20:21] Finish extracting weighted average features. △M: +0B. △T: 31.8 seconds.\n",
      "[09:20:23] Finish merging all features. △M: +0B. △T: 1.9 seconds.\n",
      "[09:20:25] Finish saving features. △M: +0B. △T: 1.6 seconds.\n",
      "Start processing 'aid' x 'interest2'\n",
      "[09:20:33] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:24:06] Finish extracting quantile features. △M: +4.0KB. △T: 3.5 minutes.\n",
      "[09:24:42] Finish extracting statistical features. △M: +0B. △T: 36.1 seconds.\n",
      "[09:25:18] Finish extracting statistical features. △M: +0B. △T: 36.1 seconds.\n",
      "[09:25:34] Finish extracting correlated statistical features. △M: +0B. △T: 16.0 seconds.\n",
      "[09:26:01] Finish extracting weighted average features. △M: +0B. △T: 26.8 seconds.\n",
      "[09:26:03] Finish merging all features. △M: +0B. △T: 1.8 seconds.\n",
      "[09:26:04] Finish saving features. △M: +0B. △T: 1.4 seconds.\n",
      "Start processing 'creativeSize' x 'interest2'\n",
      "[09:26:11] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:29:47] Finish extracting quantile features. △M: +4.0KB. △T: 3.6 minutes.\n",
      "[09:30:22] Finish extracting statistical features. △M: +0B. △T: 35.0 seconds.\n",
      "[09:30:56] Finish extracting statistical features. △M: +0B. △T: 34.9 seconds.\n",
      "[09:31:12] Finish extracting correlated statistical features. △M: +0B. △T: 15.2 seconds.\n",
      "[09:31:38] Finish extracting weighted average features. △M: +0B. △T: 26.1 seconds.\n",
      "[09:31:40] Finish merging all features. △M: +0B. △T: 1.8 seconds.\n",
      "[09:31:41] Finish saving features. △M: +0B. △T: 1.4 seconds.\n",
      "Start processing 'campaignId' x 'interest4'\n",
      "[09:31:46] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:35:19] Finish extracting quantile features. △M: +4.0KB. △T: 3.6 minutes.\n",
      "[09:35:52] Finish extracting statistical features. △M: +0B. △T: 32.8 seconds.\n",
      "[09:36:25] Finish extracting statistical features. △M: +0B. △T: 33.1 seconds.\n",
      "[09:36:39] Finish extracting correlated statistical features. △M: +0B. △T: 14.2 seconds.\n",
      "[09:37:03] Finish extracting weighted average features. △M: +0B. △T: 23.6 seconds.\n",
      "[09:37:05] Finish merging all features. △M: +0B. △T: 1.6 seconds.\n",
      "[09:37:06] Finish saving features. △M: +0B. △T: 1.3 seconds.\n",
      "Start processing 'aid' x 'interest5'\n",
      "[09:37:20] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:40:56] Finish extracting quantile features. △M: +4.0KB. △T: 3.6 minutes.\n",
      "[09:41:34] Finish extracting statistical features. △M: +0B. △T: 37.7 seconds.\n",
      "[09:42:12] Finish extracting statistical features. △M: +0B. △T: 37.8 seconds.\n",
      "[09:42:30] Finish extracting correlated statistical features. △M: +0B. △T: 18.0 seconds.\n",
      "[09:43:03] Finish extracting weighted average features. △M: +0B. △T: 33.5 seconds.\n",
      "[09:43:05] Finish merging all features. △M: +0B. △T: 2.0 seconds.\n",
      "[09:43:07] Finish saving features. △M: +0B. △T: 1.6 seconds.\n",
      "Start processing 'aid' x 'ct'\n",
      "[09:43:13] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:46:45] Finish extracting quantile features. △M: +4.0KB. △T: 3.5 minutes.\n",
      "[09:47:20] Finish extracting statistical features. △M: +0B. △T: 34.7 seconds.\n",
      "[09:47:55] Finish extracting statistical features. △M: +0B. △T: 35.0 seconds.\n",
      "[09:48:10] Finish extracting correlated statistical features. △M: +0B. △T: 14.6 seconds.\n",
      "[09:48:34] Finish extracting weighted average features. △M: +0B. △T: 24.4 seconds.\n",
      "[09:48:36] Finish merging all features. △M: +0B. △T: 1.6 seconds.\n",
      "[09:48:37] Finish saving features. △M: +0B. △T: 1.3 seconds.\n",
      "Start processing 'aid' x 'os'\n",
      "[09:48:42] Finish splitting batches. △M: +0B. △T: 0.0 seconds.\n",
      "23 batches, each with 100000 rows\n",
      "[09:52:21] Finish extracting quantile features. △M: +4.0KB. △T: 3.6 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:52:54] Finish extracting statistical features. △M: +0B. △T: 33.1 seconds.\n",
      "[09:53:28] Finish extracting statistical features. △M: +0B. △T: 34.3 seconds.\n",
      "[09:53:44] Finish extracting correlated statistical features. △M: +0B. △T: 15.8 seconds.\n",
      "[09:54:10] Finish extracting weighted average features. △M: +0B. △T: 26.4 seconds.\n",
      "[09:54:12] Finish merging all features. △M: +0B. △T: 1.8 seconds.\n",
      "[09:54:14] Finish saving features. △M: +0B. △T: 1.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "for ad_feat_name, user_feat_name in pairs:\n",
    "    process_pair_test(ad_feat_name, user_feat_name, n_procs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
