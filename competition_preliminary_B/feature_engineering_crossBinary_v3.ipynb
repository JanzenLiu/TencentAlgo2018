{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../code/utils')\n",
    "sys.path.append('../code/pipeline')\n",
    "sys.path.append('../code')\n",
    "import data_utils as du\n",
    "import perf_utils as pu\n",
    "import data_jointer as dj\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_dict(dic):\n",
    "    return {v: k for k, v in dic.items()}\n",
    "\n",
    "\n",
    "def indices_to_matrix(indices, max_cols):\n",
    "    # life is short, I use my own implementation\n",
    "    n_rows = len(indices)\n",
    "    indptr = np.arange(n_rows + 1)\n",
    "    indices = np.array(indices)\n",
    "    data = np.ones(len(indices), dtype=np.int8)\n",
    "    matrix = sparse.csr_matrix((data, indices, indptr), shape=(n_rows, max_cols) ,dtype=np.int8)\n",
    "\n",
    "    del indptr\n",
    "    del indices\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def cross_vectorize(df, feat_names, add_prefix=True):\n",
    "    assert len(feat_names) == 2  # only 2 degree crossing is supported now\n",
    "    \n",
    "    # get unique values\n",
    "    feat1_name = feat_names[0]\n",
    "    feat2_name = feat_names[1]\n",
    "    feat1_vals = df[feat1_name].unique()\n",
    "    feat2_vals = df[feat2_name].unique()\n",
    "\n",
    "    # count number of unique values\n",
    "    feat1_nunique = len(feat1_vals)\n",
    "    feat2_nunique = len(feat2_vals)\n",
    "    num_combinations  = feat1_nunique * feat2_nunique\n",
    "\n",
    "    # get index base and offset for unique values\n",
    "    feat1_to_index = dj.list_to_dict(feat1_vals, feat2_nunique)\n",
    "    feat2_to_index = dj.list_to_dict(feat2_vals)\n",
    "\n",
    "    # get indices\n",
    "    indices1 = df[feat1_name].map(feat1_to_index)\n",
    "    indices2 = df[feat2_name].map(feat2_to_index)\n",
    "    indices = indices1 + indices2\n",
    "    assert indices.nunique() >= max(feat1_nunique, feat2_nunique)\n",
    "    assert indices.nunique() <= feat1_nunique * feat2_nunique\n",
    "    \n",
    "    # get column names\n",
    "    index_to_feat1 = inverse_dict(feat1_to_index)\n",
    "    index_to_feat2 = inverse_dict(feat2_to_index)\n",
    "    col_names = []\n",
    "    if not add_prefix:\n",
    "        for i in range(0, num_combinations, feat2_nunique):\n",
    "            feat1_val = index_to_feat1[i]\n",
    "            col_names += [\"{}x{}\".format(feat1_val, index_to_feat2[j]) for j in range(feat2_nunique)]\n",
    "    else:\n",
    "        for i in range(0, num_combinations, feat2_nunique):\n",
    "            feat1_val = index_to_feat1[i]\n",
    "            col_names += [\"{}_{}_x_{}_{}\".format(feat1_name, feat1_val, feat2_name, index_to_feat2[j]) \n",
    "                          for j in range(feat2_nunique)]\n",
    "    \n",
    "    # release memory and collect garbage\n",
    "    del feat1_to_index\n",
    "    del feat2_to_index\n",
    "    del index_to_feat1\n",
    "    del index_to_feat2\n",
    "    del indices1\n",
    "    del indices2\n",
    "    gc.collect()\n",
    "\n",
    "    # construct sparse matrix\n",
    "    matrix = indices_to_matrix(indices, num_combinations)\n",
    "    assert matrix.shape[0] == df.shape[0]\n",
    "    assert matrix.shape[1] == df[feat1_name].nunique() * df[feat2_name].nunique()\n",
    "    return matrix, col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = os.path.join(config.PRELIM_NLP_COUNT_DATA_DIR, \"user_cross/\")\n",
    "\n",
    "def cross_binary_path(feat_name1, feat_name2, prefix=\"train\", create=True):\n",
    "    folder = out_folder\n",
    "    file = \"{}.['{}'x'{}'].binary.pkl\".format(prefix, feat_name1, feat_name2)\n",
    "    path = os.path.join(folder, file)\n",
    "    if create:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 8798814\n",
      "Test Size: 2265879\n",
      "Concatenated Data Shape: (11064693, 3)\n"
     ]
    }
   ],
   "source": [
    "user_one_feat_names = config.USER_SINGLE_FEAT_NAMES\n",
    "\n",
    "df_train = du.load_raw_data(\"train\")\n",
    "df_test = du.load_raw_data(\"test2\")\n",
    "\n",
    "train_size = df_train.shape[0]\n",
    "test_size = df_test.shape[0]\n",
    "df_all = pd.concat([df_train, df_test], ignore_index=True)\n",
    "print(\"Train Size: {}\".format(train_size))\n",
    "print(\"Test Size: {}\".format(test_size))\n",
    "print(\"Concatenated Data Shape: {}\".format(df_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:01:41] Finish loading and joining 'age'. △M: +175.37MB. △T: 9.9 seconds.\n",
      "[06:01:51] Finish loading and joining 'gender'. △M: +84.43MB. △T: 10.3 seconds.\n",
      "[06:02:03] Finish loading and joining 'education'. △M: +84.42MB. △T: 11.5 seconds.\n",
      "[06:02:14] Finish loading and joining 'consumptionAbility'. △M: +84.42MB. △T: 11.4 seconds.\n",
      "[06:02:27] Finish loading and joining 'LBS'. △M: +85.65MB. △T: 12.2 seconds.\n",
      "[06:02:38] Finish loading and joining 'carrier'. △M: +84.42MB. △T: 11.6 seconds.\n",
      "[06:02:50] Finish loading and joining 'house'. △M: +84.42MB. △T: 11.7 seconds.\n"
     ]
    }
   ],
   "source": [
    "user_jointer = dj.PandasPandasJointer(\"uid\")\n",
    "\n",
    "for user_feat_name in user_one_feat_names:\n",
    "    with pu.profiler(\"loading and joining '{}'\".format(user_feat_name)):\n",
    "        df_feat = du.load_user_feature(user_feat_name).fillna(\"[nan]\")  # load user feature\n",
    "        df_all = user_jointer.join(df_all, df_feat)  # join user feature\n",
    "        \n",
    "        del df_feat\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:02:54] Finish vectorizing and saving 'carrier'x'LBS' binary. △M: -11.3MB. △T: 3.9 seconds.\n",
      "[06:02:57] Finish vectorizing and saving 'house'x'LBS' binary. △M: -24.0KB. △T: 3.5 seconds.\n",
      "[06:03:01] Finish vectorizing and saving 'gender'x'LBS' binary. △M: +8.0KB. △T: 3.4 seconds.\n"
     ]
    }
   ],
   "source": [
    "pairs = [(\"LBS\", \"carrier\"), (\"LBS\", \"house\"), (\"LBS\", \"gender\")]\n",
    "\n",
    "\n",
    "for ad_feat_name, user_feat_name in pairs:\n",
    "    with pu.profiler(\"vectorizing and saving '{}'x'{}' binary\".format(user_feat_name, ad_feat_name)):\n",
    "        # get matrix and names for matrix columns\n",
    "        matrix, col_names = cross_vectorize(df_all, [ad_feat_name, user_feat_name])\n",
    "\n",
    "        # save train matrix\n",
    "        out_path = cross_binary_path(ad_feat_name, user_feat_name, prefix=\"train\")\n",
    "        matrix_train = matrix[:train_size, :]\n",
    "        du.save_pickle((col_names, matrix_train), out_path)\n",
    "        del matrix_train\n",
    "        gc.collect()\n",
    "\n",
    "        # save test matrix\n",
    "        out_path = cross_binary_path(ad_feat_name, user_feat_name, prefix=\"test2\")\n",
    "        matrix_test = matrix[train_size:, :]\n",
    "        du.save_pickle((col_names, matrix_test), out_path)\n",
    "        del matrix_test\n",
    "        gc.collect()\n",
    "\n",
    "        # release memory and clean garbage\n",
    "        del matrix\n",
    "        del col_names\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
