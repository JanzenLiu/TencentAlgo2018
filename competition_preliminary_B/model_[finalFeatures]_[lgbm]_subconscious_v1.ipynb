{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from contextlib import redirect_stdout\n",
    "import scipy.sparse as sparse\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../../../code/pipeline/')\n",
    "sys.path.append('../../../code/utils/')\n",
    "sys.path.append('../../../code/')\n",
    "import data_pipeline as dp\n",
    "import data_utils as du\n",
    "import perf_utils as pu\n",
    "import eval_utils as eu\n",
    "import io_utils as iu\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multiple data and stack them together\n",
    "input_folder = os.path.join(config.DATA_DIR, \"input_final\")\n",
    "dm = dp.DataManager(input_folder)\n",
    "\n",
    "lc_v1 = dm.build_data('aggregate.listCount_v1')\n",
    "lc_v2 = dm.build_data('aggregate.listCount_v2')\n",
    "clk_v1 = dm.build_data('clickStats.crossWordCount_v1')\n",
    "clk_v2 = dm.build_data('clickStats.crossWordCount_v2')\n",
    "stack_cross_v1 = dm.build_data('stacking.lr.crossWordCount_v1')\n",
    "stack_cross_v1_all = dm.build_data('stacking.lr.crossWordCount_v1_all')\n",
    "stack_cross_v2 = dm.build_data('stacking.lr.crossWordCount_v2')\n",
    "stack_cross_v3 = dm.build_data('stacking.lr.crossWordCount_v3')\n",
    "stack_cross_v3_all = dm.build_data('stacking.lr.crossWordCount_v3_all')\n",
    "stack_emb = dm.build_data('stacking.lr.wordEmbedding')\n",
    "stack_tfidf = dm.build_data('stacking.lr.wordTfIdf')\n",
    "raw = dm.build_data('raw.wordCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_loader = dp.DataUnion(lc_v1, lc_v2, \n",
    "                            clk_v1, clk_v2,\n",
    "                            stack_cross_v1, stack_cross_v1_all, \n",
    "                            stack_cross_v2, \n",
    "                            stack_cross_v3, stack_cross_v3_all, \n",
    "                            stack_emb, \n",
    "                            stack_tfidf, \n",
    "                            raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pu.profiler(\"loading training data\"):\n",
    "#     cols_train, X_tv = union_loader.load(\"train\")\n",
    "#     X_tv = sparse.csr_matrix(X_tv)\n",
    "#     gc.collect()\n",
    "# print(\"Train Data Shape: {}\".format(X_tv.shape))\n",
    "# print(\"Train Column Numbers: {}\".format(len(cols_train)))\n",
    "\n",
    "df_train = du.load_raw_data(\"train\")\n",
    "y = df_train['label'].values.copy()\n",
    "y = (y + 1) / 2  # -1, 1 -> 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:30:44] Finish splitting train/valid set. △M: -10.52GB. △T: 4.0 minutes.\n",
      "Training Set Size: (7039051, 419818)\n",
      "Validation Set Size: (1759763, 419818)\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=2018)  # for reproducibility\n",
    "split_indices = [(train_index, valid_index) for train_index, valid_index in sss.split(df_train, y)]\n",
    "# n_splits = 5  # use 3 instead of 5 to save time\n",
    "# skf = StratifiedKFold(n_splits=n_splits, random_state=2018)\n",
    "# split_indices = [(train_index, valid_index) for train_index, valid_index in skf.split(df_train, y)]\n",
    "\n",
    "aids = df_train['aid'].values\n",
    "with pu.profiler(\"splitting train/valid set\"):\n",
    "    train_index, valid_index = split_indices[0]\n",
    "    X_train, X_valid = X_tv[train_index, :], X_tv[valid_index, :]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    aids_train, aids_valid = aids[train_index], aids[valid_index]\n",
    "    assert X_train.shape[0] + X_valid.shape[0] == X_tv.shape[0]\n",
    "    \n",
    "    del X_tv\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"Training Set Size: {}\".format(X_train.shape))\n",
    "print(\"Validation Set Size: {}\".format(X_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:30:17] Finish preparing LightGBM data. △M: +8.06GB. △T: 1.4 minutes.\n"
     ]
    }
   ],
   "source": [
    "with pu.profiler(\"preparing LightGBM data\"):\n",
    "    lgb_train = lgb.Dataset(X_train.astype(np.float32), y_train)\n",
    "    del X_train  # to save memory\n",
    "    gc.collect()\n",
    "    \n",
    "    lgb_valid = lgb.Dataset(X_valid.astype(np.float32), y_valid)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-874306fba5a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                      \u001b[0mfeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_auc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                      \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                      early_stopping_rounds=50)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_sets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_train\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m         \"\"\"\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_data_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m                 \u001b[0mcur_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m             \u001b[0mfeval_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_higher_better\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeval_ret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/cheng/Desktop/Competitons/TencentAlgo2018/code/utils/eval_utils.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(y_pred, lgbm_data)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgbm_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain_selectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_selectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalid_selectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_selectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/cheng/Desktop/Competitons/TencentAlgo2018/code/utils/eval_utils.py\u001b[0m in \u001b[0;36monline_auc\u001b[0;34m(selector, y_true, y_pred, selector_name, ret_verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my_true_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0my_pred_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0maucs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret_verbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    275\u001b[0m     return _average_binary_score(\n\u001b[1;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: continuous format is not supported"
     ]
    }
   ],
   "source": [
    "# v1: \n",
    "version_name = \"final/subconscious_v1\"\n",
    "log_folder = os.path.join(config.LOG_DIR, 'lgbm/{}'.format(version_name))\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 64,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.001,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'verbose': 0\n",
    "}\n",
    "num_rounds = 800000\n",
    "\n",
    "log_file = 'params.json'\n",
    "log_path = os.path.join(log_folder, log_file)\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "log_file = 'log.txt'\n",
    "log_path = os.path.join(log_folder, log_file)\n",
    "eval_auc = eu.build_lightgbm_online_auc_eval(aids_train, aids_valid)\n",
    "with iu.DuplicatedLogger(log_path):\n",
    "    lgbm = lgb.train(params,\n",
    "                     lgb_train,\n",
    "                     num_boost_round=num_rounds,\n",
    "                     valid_sets=[lgb_train, lgb_valid], \n",
    "                     valid_names=['train', 'valid'],\n",
    "                     feval = eval_auc,\n",
    "                     verbose_eval=50,\n",
    "                     early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_file = 'feature_importance.csv'\n",
    "log_path = os.path.join(log_folder, log_file)\n",
    "\n",
    "df_feature_importance = pd.DataFrame({\"feature\": cols_train, \"importance\": lgbm.feature_importance()})\n",
    "df_feature_importance = df_feature_importance.sort_values(\"importance\", ascending=False)\n",
    "df_feature_importance.to_csv(log_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online AUC: 0.725787\n",
      "Simple AUC: 0.731557\n"
     ]
    }
   ],
   "source": [
    "df_valid = df_train.iloc[valid_index]\n",
    "proba_valid = lgbm.predict(X_valid.astype(np.float32))\n",
    "df_score = eu.online_auc(df_valid['aid'], y_valid, proba_valid, ret_verbose=True)\n",
    "\n",
    "online_auc = df_score['auc'].mean()\n",
    "simple_auc = metrics.roc_auc_score(y_valid, proba_valid)\n",
    "print(\"Online AUC: {:.6f}\".format(online_auc))\n",
    "print(\"Simple AUC: {:.6f}\".format(simple_auc))\n",
    "\n",
    "log_file = 'online_auc.csv'\n",
    "log_path = os.path.join(log_folder, log_file)\n",
    "df_score.rename(columns={'selector': 'aid'}, inplace=True)\n",
    "df_score = df_score[['aid', 'auc']]  # sort columns\n",
    "df_score = df_score.sort_values(\"auc\", ascending=False)\n",
    "df_score.to_csv(log_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:40:42] Finish cleaning memory. △M: -5.66GB. △T: 1.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "with pu.profiler(\"cleaning memory\"):\n",
    "    del lgb_train\n",
    "    del lgb_valid\n",
    "    # del X_train\n",
    "    del X_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:38:37] Finish loading testing data. △M: -661.99MB. △T: 8.3 seconds.\n",
      "Test Data Shape: (2265879, 419701)\n",
      "Test Column Numbers: 419702\n",
      "[04:38:59] Finish making prediction on testing set. △M: +0B. △T: 19.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "with pu.profiler(\"loading testing data\"):\n",
    "    cols_test, X_test = union_loader.load(\"test2\")\n",
    "    cols_train_set = set(cols_train)\n",
    "    mask = [i for i, col in enumerate(cols_test) if col in cols_train_set]\n",
    "    X_test = sparse.csr_matrix(X_test[:, mask])\n",
    "    gc.collect()\n",
    "print(\"Test Data Shape: {}\".format(X_test.shape))\n",
    "print(\"Test Column Numbers: {}\".format(len(cols_test)))\n",
    "\n",
    "df_test = du.load_raw_data(\"test2\")\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "with pu.profiler(\"making prediction on testing set\"):\n",
    "    proba_test = lgbm.predict(X_test)\n",
    "    assert len(proba_test.shape) == 1\n",
    "    assert proba_test.shape[0] == df_test.shape[0]\n",
    "    \n",
    "subm_folder = '../../../subm/lgbm/{}'.format(version_name)\n",
    "subm_file = 'submission.csv'\n",
    "subm_path = os.path.join(subm_folder, subm_file)\n",
    "os.makedirs(subm_folder, exist_ok=True)\n",
    "\n",
    "subm = df_test.copy()\n",
    "subm[\"score\"] = proba_test\n",
    "subm.to_csv(subm_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419701"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
