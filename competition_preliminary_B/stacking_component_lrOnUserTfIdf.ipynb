{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../code/pipeline/')\n",
    "sys.path.append('../code/utils/')\n",
    "sys.path.append('../code/')\n",
    "import data_pipeline as dp\n",
    "import data_utils as du\n",
    "import perf_utils as pu\n",
    "import eval_utils as eu\n",
    "import io_utils as iu\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:10:25] Finish loading training data. △M: +4.75GB. △T: 5.9 seconds.\n",
      "Train Data Shape: (8798814, 419220)\n",
      "Train Column Numbers: 419220\n"
     ]
    }
   ],
   "source": [
    "# load multiple data and stack them together\n",
    "dm = dp.DataManager(config.INPUT_DIR)\n",
    "bin_loader = dm.build_data(\"user\", \"tfidf\")\n",
    "union_loader = dp.DataUnion(bin_loader)\n",
    "\n",
    "with pu.profiler(\"loading training data\"):\n",
    "    cols_train, X_tv = union_loader.load(\"train\")\n",
    "    X_tv = sparse.csr_matrix(X_tv)\n",
    "    gc.collect()\n",
    "print(\"Train Data Shape: {}\".format(X_tv.shape))\n",
    "print(\"Train Column Numbers: {}\".format(len(cols_train)))\n",
    "\n",
    "df_train = du.load_raw_data(\"train\")\n",
    "y = df_train['label'].values.copy()\n",
    "y = (y + 1) / 2  # -1, 1 -> 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:11:33] Finish loading testing data. △M: +1.24GB. △T: 2.4 seconds.\n",
      "Test Data Shape: (2265879, 419220)\n",
      "Test Column Numbers: 419220\n"
     ]
    }
   ],
   "source": [
    "with pu.profiler(\"loading testing data\"):\n",
    "    cols_test, X_test = union_loader.load(\"test2\")\n",
    "    X_test = sparse.csr_matrix(X_test)\n",
    "    gc.collect()\n",
    "print(\"Test Data Shape: {}\".format(X_test.shape))\n",
    "print(\"Test Column Numbers: {}\".format(len(cols_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = du.load_raw_data(\"test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 2  # 3? 5? Don't know which one will be better\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=2018)  # for reproducibility\n",
    "split_indices = [(train_index, valid_index) for train_index, valid_index in skf.split(df_train, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_tv = np.zeros(X_tv.shape[0])\n",
    "stack_test = np.zeros((X_test.shape[0], n_splits))\n",
    "scores = np.zeros(n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:23:26] Finish fitting LR (fold 1/2). △M: -7.44GB. △T: 1.3 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:02:36] Finish fitting LR (fold 2/2). △M: +8.0KB. △T: 1.6 hours.\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, valid_index) in enumerate(split_indices):\n",
    "    ### given a splitting ###\n",
    "    # split train/valid sets\n",
    "    X_train, y_train = X_tv[train_index], y[train_index]\n",
    "    X_valid, y_valid = X_tv[valid_index], y[valid_index]\n",
    "\n",
    "    # fit LR\n",
    "    with pu.profiler(\"fitting LR (fold {}/{})\".format(i + 1, n_splits)):\n",
    "        lr = LogisticRegression(solver=\"newton-cg\", n_jobs=-1)  # use default setting: penalty='l2' and C=1\n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "    # make prediction for validation set\n",
    "    proba_valid = lr.predict_proba(X_valid)[:, 1]\n",
    "    stack_tv[valid_index] = proba_valid\n",
    "\n",
    "    # make prediction for testing set\n",
    "    proba_test = lr.predict_proba(X_test)[:, 1]\n",
    "    stack_test[:, i] = proba_test\n",
    "\n",
    "    # calculate scores\n",
    "    auc = metrics.roc_auc_score(y_valid, proba_valid)\n",
    "    scores[i] = auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.659131(+/-0.000112)\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall AUC: {:.6f}(+/-{:.3g})\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:00:54] Finish saving train prediction. △M: -3.73GB. △T: 1.0 seconds.\n",
      "Saved Shape: (8798814, 1)\n",
      "[11:00:54] Finish saving test prediction. △M: +0B. △T: 0.1 seconds.\n",
      "Saved Shape: (2265879, 1)\n"
     ]
    }
   ],
   "source": [
    "out_folder = os.path.join(config.DATA_DIR, \"stacking/lr\")\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "out_file = \"train.userTfIdf.pkl\"\n",
    "out_path = os.path.join(out_folder, out_file)\n",
    "with pu.profiler(\"saving train prediction\"):\n",
    "    col_names = ['stackProba_LR_userTfIdf']\n",
    "    data_tv = stack_tv.reshape((-1, 1)).astype(np.float32)\n",
    "    du.save_pickle((col_names, data_tv), out_path)\n",
    "    gc.collect()\n",
    "print(\"Saved Shape: {}\".format(data_tv.shape))\n",
    "    \n",
    "out_file = \"test2.userTfIdf.pkl\"\n",
    "out_path = os.path.join(out_folder, out_file)\n",
    "with pu.profiler(\"saving test prediction\"):\n",
    "    data_test = stack_test.mean(axis=1).reshape((-1, 1)).astype(np.float32)\n",
    "    du.save_pickle((col_names, data_test), out_path)\n",
    "    gc.collect()\n",
    "print(\"Saved Shape: {}\".format(data_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
