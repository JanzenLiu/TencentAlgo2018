{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from time import gmtime, strftime\n",
    "import gc\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../../../code/utils')\n",
    "sys.path.append('../../../code')\n",
    "import perf_utils as pu\n",
    "import data_utils as du\n",
    "import io_utils as iu\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading lines:   0%|          | 47297/11420039 [00:00<00:24, 472857.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:21:12] Finish counting lines. △M: +773.91MB. △T: 24.5 seconds.\n",
      "11420039 lines in userFeature.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading lines: 100%|██████████| 11420039/11420039 [00:34<00:00, 328740.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage at this moment: 4.98GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DATA_DIR = '../../../data/raw/preliminary_contest_data/'\n",
    "DATA_DIR = '/mnt/d/DataShortcut/raw/preliminary_contest_data/'\n",
    "USER_DATA_FILE = 'userFeature.data'\n",
    "USER_DATA_PATH = os.path.join(DATA_DIR, USER_DATA_FILE)\n",
    "assert os.path.exists(USER_DATA_PATH)\n",
    "\n",
    "with pu.profiler(\"counting lines\"):\n",
    "    line_counts = iu.count_file_lines(USER_DATA_PATH)\n",
    "print(\"{} lines in userFeature.data\".format(line_counts))\n",
    "\n",
    "lines = []\n",
    "with open(USER_DATA_PATH) as f:\n",
    "    for i in tqdm(range(line_counts), desc=\"loading lines\"):\n",
    "        line = f.readline().strip()\n",
    "        lines.append(line)\n",
    "print(\"Memory Usage at this moment: {}\".format(pu.get_memory_str()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = config.USER_FEAT_NAMES\n",
    "\n",
    "\n",
    "def split_list(lst, n_split=4):\n",
    "    n_total = len(lst)\n",
    "    step = int(np.ceil(n_total / n_split))\n",
    "    splits = []\n",
    "    for offset in range(0, n_total, step):\n",
    "        splits.append(lst[offset:offset + step])\n",
    "    return splits\n",
    "\n",
    "\n",
    "def fast_get_word_from_lines(lines):\n",
    "    vocabs = {feat_name: set() for feat_name in feat_names}\n",
    "    n_lines = len(lines)\n",
    "    for line in lines:\n",
    "        for feat in line.split(\"|\")[1:]:\n",
    "            arr = feat.split(\" \")\n",
    "            key = arr[0]\n",
    "            vals = arr[1:]\n",
    "            vocabs[key].update(vals)\n",
    "    return vocabs\n",
    "\n",
    "\n",
    "def merge_set_dicts(set_dict_list):\n",
    "    final_set_dict = {}\n",
    "    for feat_name in feat_names:\n",
    "        final_set = set()\n",
    "        for set_dict in set_dict_list:\n",
    "            final_set.update(set_dict[feat_name])\n",
    "        final_set_dict[feat_name] = final_set\n",
    "    return final_set_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_get_words(lines, n_procs=None, n_batches=None):\n",
    "    n_procs = mp.cpu_count() if n_procs is None else n_procs\n",
    "    n_batches = mp.cpu_count() if n_batches is None else n_batches\n",
    "    pool = mp.Pool(processes=n_procs)\n",
    "    results = [pool.apply_async(fast_get_word_from_lines, (batch, )) for batch in split_list(lines, n_procs)]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    sets_list = [result.get() for result in results]\n",
    "    final_set_dict = merge_set_dicts(sets_list)\n",
    "    return final_set_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:22:21] Batch 1/100 Done.\n",
      "[09:22:29] Batch 2/100 Done.\n",
      "[09:22:36] Batch 3/100 Done.\n",
      "[09:22:44] Batch 4/100 Done.\n",
      "[09:22:50] Batch 5/100 Done.\n",
      "[09:22:58] Batch 6/100 Done.\n",
      "[09:23:06] Batch 7/100 Done.\n",
      "[09:23:14] Batch 8/100 Done.\n",
      "[09:23:21] Batch 9/100 Done.\n",
      "[09:23:30] Batch 10/100 Done.\n",
      "[09:23:38] Batch 11/100 Done.\n",
      "[09:23:45] Batch 12/100 Done.\n",
      "[09:23:53] Batch 13/100 Done.\n",
      "[09:24:01] Batch 14/100 Done.\n",
      "[09:24:08] Batch 15/100 Done.\n",
      "[09:24:17] Batch 16/100 Done.\n",
      "[09:24:25] Batch 17/100 Done.\n",
      "[09:24:32] Batch 18/100 Done.\n",
      "[09:24:40] Batch 19/100 Done.\n",
      "[09:24:48] Batch 20/100 Done.\n",
      "[09:24:56] Batch 21/100 Done.\n",
      "[09:25:04] Batch 22/100 Done.\n",
      "[09:25:11] Batch 23/100 Done.\n",
      "[09:25:19] Batch 24/100 Done.\n",
      "[09:25:26] Batch 25/100 Done.\n",
      "[09:25:35] Batch 26/100 Done.\n",
      "[09:25:43] Batch 27/100 Done.\n",
      "[09:25:51] Batch 28/100 Done.\n",
      "[09:26:00] Batch 29/100 Done.\n",
      "[09:26:08] Batch 30/100 Done.\n",
      "[09:26:16] Batch 31/100 Done.\n",
      "[09:26:23] Batch 32/100 Done.\n",
      "[09:26:31] Batch 33/100 Done.\n",
      "[09:26:40] Batch 34/100 Done.\n",
      "[09:26:48] Batch 35/100 Done.\n",
      "[09:26:57] Batch 36/100 Done.\n",
      "[09:27:05] Batch 37/100 Done.\n",
      "[09:27:13] Batch 38/100 Done.\n",
      "[09:27:21] Batch 39/100 Done.\n",
      "[09:27:29] Batch 40/100 Done.\n",
      "[09:27:37] Batch 41/100 Done.\n",
      "[09:27:45] Batch 42/100 Done.\n",
      "[09:27:52] Batch 43/100 Done.\n",
      "[09:28:01] Batch 44/100 Done.\n",
      "[09:28:09] Batch 45/100 Done.\n",
      "[09:28:17] Batch 46/100 Done.\n",
      "[09:28:25] Batch 47/100 Done.\n",
      "[09:28:33] Batch 48/100 Done.\n",
      "[09:28:41] Batch 49/100 Done.\n",
      "[09:28:50] Batch 50/100 Done.\n",
      "[09:29:00] Batch 51/100 Done.\n",
      "[09:29:08] Batch 52/100 Done.\n",
      "[09:29:16] Batch 53/100 Done.\n",
      "[09:29:25] Batch 54/100 Done.\n",
      "[09:29:33] Batch 55/100 Done.\n",
      "[09:29:42] Batch 56/100 Done.\n",
      "[09:29:52] Batch 57/100 Done.\n",
      "[09:30:00] Batch 58/100 Done.\n",
      "[09:30:08] Batch 59/100 Done.\n",
      "[09:30:17] Batch 60/100 Done.\n",
      "[09:30:26] Batch 61/100 Done.\n",
      "[09:30:34] Batch 62/100 Done.\n",
      "[09:30:42] Batch 63/100 Done.\n",
      "[09:30:49] Batch 64/100 Done.\n",
      "[09:30:57] Batch 65/100 Done.\n",
      "[09:31:07] Batch 66/100 Done.\n",
      "[09:31:19] Batch 67/100 Done.\n",
      "[09:31:28] Batch 68/100 Done.\n",
      "[09:31:37] Batch 69/100 Done.\n",
      "[09:31:45] Batch 70/100 Done.\n",
      "[09:31:54] Batch 71/100 Done.\n",
      "[09:32:04] Batch 72/100 Done.\n",
      "[09:32:14] Batch 73/100 Done.\n",
      "[09:32:22] Batch 74/100 Done.\n",
      "[09:32:31] Batch 75/100 Done.\n",
      "[09:32:40] Batch 76/100 Done.\n",
      "[09:32:49] Batch 77/100 Done.\n",
      "[09:32:58] Batch 78/100 Done.\n",
      "[09:33:08] Batch 79/100 Done.\n",
      "[09:33:17] Batch 80/100 Done.\n",
      "[09:33:26] Batch 81/100 Done.\n",
      "[09:33:34] Batch 82/100 Done.\n",
      "[09:33:43] Batch 83/100 Done.\n",
      "[09:33:54] Batch 84/100 Done.\n",
      "[09:34:04] Batch 85/100 Done.\n",
      "[09:34:13] Batch 86/100 Done.\n",
      "[09:34:22] Batch 87/100 Done.\n",
      "[09:34:32] Batch 88/100 Done.\n",
      "[09:34:40] Batch 89/100 Done.\n",
      "[09:34:49] Batch 90/100 Done.\n",
      "[09:34:58] Batch 91/100 Done.\n",
      "[09:35:07] Batch 92/100 Done.\n",
      "[09:35:16] Batch 93/100 Done.\n",
      "[09:35:25] Batch 94/100 Done.\n",
      "[09:35:34] Batch 95/100 Done.\n",
      "[09:35:44] Batch 96/100 Done.\n",
      "[09:35:52] Batch 97/100 Done.\n",
      "[09:36:02] Batch 98/100 Done.\n",
      "[09:36:12] Batch 99/100 Done.\n",
      "[09:36:21] Batch 100/100 Done.\n",
      "[09:36:23] Finish merging results. △M: +18.0MB. △T: 2.0 seconds.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_memory_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-87e23795a0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merging results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvocab_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_set_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Memory Usage at this moment: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_memory_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_memory_str' is not defined"
     ]
    }
   ],
   "source": [
    "n_batches = 100\n",
    "line_batches = split_list(lines, n_batches)\n",
    "vocab_dicts = []\n",
    "for i, line_batch in enumerate(line_batches):\n",
    "    vocab_dicts.append(batch_get_words(line_batch, 8))\n",
    "    print(\"[{}] Batch {}/{} Done.\".format(pu.get_time_str(), i+1, n_batches))\n",
    "\n",
    "with pu.profiler(\"merging results\"):\n",
    "    vocab_dict = merge_set_dicts(vocab_dicts)\n",
    "print(\"Memory Usage at this moment: {}\".format(pu.get_memory_str()))                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_folder = '../../../data/vocabulary/preliminary_contest_data/'\n",
    "os.makedirs(vocab_folder, exist_ok=True)\n",
    "for feat_name, vocab in vocab_dict.items():\n",
    "    vocab_file = \"userFeature.[featureName='{}'].pkl\".format(feat_name)\n",
    "    vocab_path = os.path.join(vocab_folder, vocab_file)\n",
    "    du.save_pickle(list(vocab), vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
