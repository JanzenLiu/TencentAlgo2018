{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import sys\n",
    "sys.path.append('../../../code/utils/')\n",
    "sys.path.append('../../../code/feature/')\n",
    "import data_utils as du\n",
    "import perf_utils as pu\n",
    "import gc\n",
    "from scipy.sparse import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20180429)    # reproducible\n",
    "# Hyper Parameters\n",
    "EPOCH = 100                                       #Haven't figured out, let these just be here\n",
    "BATCH_SIZE = 256\n",
    "LR = 5e-5         # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_to_tensor(csr_matrix, size):\n",
    "    data = csr_matrix.data\n",
    "    indices = csr_matrix.indices\n",
    "    \n",
    "    # http://pytorch.org/docs/stable/sparse.html\n",
    "    i = torch.LongTensor([[0, num] for num in indices], device=device)\n",
    "    v = torch.FloatTensor(data.astype(np.float), device=device)\n",
    "    result_tensor = torch.sparse.FloatTensor(i.t(), v, size, device=device).to_dense()\n",
    "    return result_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you should convert data into formats that torch accepts\n",
    "# Make a dataloader\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.label = y\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        # convert self.input[index] to tensor\n",
    "        input_item = self.input[index]\n",
    "        x = csr_to_tensor(input_item, torch.Size([1,self.input.shape[1]]))\n",
    "        \n",
    "        # convert self.label[index] to tensor\n",
    "        y = torch.tensor(self.label[index])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we should probably define the auto-encoder\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 32),\n",
    "        ).to(device)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, input_size),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x.to(device))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 728)\t1\n",
      "  (0, 893)\t1\n",
      "  (0, 908)\t1\n",
      "  (0, 923)\t1\n",
      "  (0, 941)\t1\n",
      "  (0, 985)\t1\n",
      "  (0, 986)\t1\n",
      "  (0, 1004)\t1\n",
      "  (0, 1092)\t1\n",
      "  (0, 1103)\t1\n",
      "  (0, 1114)\t1\n",
      "  (0, 1251)\t1\n",
      "  (0, 28269)\t1\n",
      "  (0, 114175)\t1\n",
      "  (0, 122177)\t1\n",
      "  (0, 177443)\t1\n",
      "  (0, 177584)\t1\n",
      "  (0, 310357)\t1\n",
      "  (0, 322279)\t1\n",
      "  (0, 323861)\t1\n",
      "  (0, 324111)\t1\n",
      "  (0, 324797)\t1\n",
      "  (0, 329213)\t1\n",
      "  (0, 330004)\t1\n",
      "  (0, 342260)\t1\n",
      "  (0, 348133)\t1\n",
      "  (0, 412989)\t1\n",
      "  (0, 419204)\t1\n",
      "  (0, 419206)\t1\n",
      "  (0, 419260)\t1\n",
      "  (0, 419395)\t1\n",
      "  (0, 419567)\t1\n",
      "  (0, 419658)\t1\n",
      "  (0, 419775)\t1\n",
      "  (0, 419790)\t1\n",
      "  (0, 419825)\t1\n",
      "  (0, 419859)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df_train = du.load_raw_data(\"train\")\n",
    "gc.collect()\n",
    "\n",
    "# train_X: (8798814, 374251)\n",
    "# train_y: (8798814,)\n",
    "\n",
    "train_X = joblib.load('../../../../../zhangez698/TencentAlgo2018/playground/Elvin/autoencoder/xxx.pkl') # on Server\n",
    "train_y = joblib.load('../../../../../zhangez698/TencentAlgo2018/playground/Elvin/autoencoder/yyy.pkl')\n",
    "#train_X, train_y = du.get_set(df_train, test = False, features_u_want = ['house', 'interest2', 'kw1', 'kw2',  'appIdInstall'], a_features_u_want = ['aid', 'productId'])\n",
    "\n",
    "print(train_X[0]) # eg. (0, 1) is the position of non-zero data whose value is 1\n",
    "del df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:31:02] Finish Setting up autoencoder. △M: +1.48GB. △T: 4.7 seconds.\n"
     ]
    }
   ],
   "source": [
    "with pu.profiler(\"Setting up autoencoder\"):\n",
    "    autoencoder = AutoEncoder(train_X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR, weight_decay = 1e-5)\n",
    "    criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], samples[256/8798814], loss:0.00593919\n",
      "end time: 14:31:05\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[2816/8798814], loss:0.00531732\n",
      "end time: 14:31:16\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[5376/8798814], loss:0.00492823\n",
      "end time: 14:31:27\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[7936/8798814], loss:0.00466196\n",
      "end time: 14:31:38\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[10496/8798814], loss:0.00445479\n",
      "end time: 14:31:50\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[13056/8798814], loss:0.00426609\n",
      "end time: 14:32:01\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[15616/8798814], loss:0.00410123\n",
      "end time: 14:32:12\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[18176/8798814], loss:0.00394280\n",
      "end time: 14:32:24\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[20736/8798814], loss:0.00379502\n",
      "end time: 14:32:35\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[23296/8798814], loss:0.00364970\n",
      "end time: 14:32:46\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[25856/8798814], loss:0.00353435\n",
      "end time: 14:32:58\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[28416/8798814], loss:0.00338907\n",
      "end time: 14:33:09\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[30976/8798814], loss:0.00327485\n",
      "end time: 14:33:21\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[33536/8798814], loss:0.00315346\n",
      "end time: 14:33:32\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[36096/8798814], loss:0.00305178\n",
      "end time: 14:33:43\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[38656/8798814], loss:0.00293246\n",
      "end time: 14:33:54\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[41216/8798814], loss:0.00284919\n",
      "end time: 14:34:06\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[43776/8798814], loss:0.00274974\n",
      "end time: 14:34:17\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[46336/8798814], loss:0.00266033\n",
      "end time: 14:34:28\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[48896/8798814], loss:0.00257309\n",
      "end time: 14:34:40\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[51456/8798814], loss:0.00248436\n",
      "end time: 14:34:51\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[54016/8798814], loss:0.00241938\n",
      "end time: 14:35:02\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[56576/8798814], loss:0.00234059\n",
      "end time: 14:35:13\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[59136/8798814], loss:0.00228349\n",
      "end time: 14:35:25\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[61696/8798814], loss:0.00221884\n",
      "end time: 14:35:36\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[64256/8798814], loss:0.00216484\n",
      "end time: 14:35:47\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[66816/8798814], loss:0.00210280\n",
      "end time: 14:35:59\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[69376/8798814], loss:0.00203937\n",
      "end time: 14:36:10\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[71936/8798814], loss:0.00199641\n",
      "end time: 14:36:21\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[74496/8798814], loss:0.00194950\n",
      "end time: 14:36:32\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[77056/8798814], loss:0.00189987\n",
      "end time: 14:36:44\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[79616/8798814], loss:0.00186150\n",
      "end time: 14:36:55\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[82176/8798814], loss:0.00181537\n",
      "end time: 14:37:07\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[84736/8798814], loss:0.00177114\n",
      "end time: 14:37:18\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[87296/8798814], loss:0.00173741\n",
      "end time: 14:37:29\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[89856/8798814], loss:0.00170807\n",
      "end time: 14:37:41\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[92416/8798814], loss:0.00167291\n",
      "end time: 14:37:52\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[94976/8798814], loss:0.00163004\n",
      "end time: 14:38:04\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[97536/8798814], loss:0.00159586\n",
      "end time: 14:38:15\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[100096/8798814], loss:0.00158082\n",
      "end time: 14:38:26\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[102656/8798814], loss:0.00154533\n",
      "end time: 14:38:38\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[105216/8798814], loss:0.00151294\n",
      "end time: 14:38:49\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[107776/8798814], loss:0.00147372\n",
      "end time: 14:39:00\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[110336/8798814], loss:0.00145417\n",
      "end time: 14:39:12\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[112896/8798814], loss:0.00142535\n",
      "end time: 14:39:23\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[115456/8798814], loss:0.00139819\n",
      "end time: 14:39:35\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[118016/8798814], loss:0.00138138\n",
      "end time: 14:39:46\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[120576/8798814], loss:0.00137342\n",
      "end time: 14:39:57\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[123136/8798814], loss:0.00133244\n",
      "end time: 14:40:09\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[125696/8798814], loss:0.00131317\n",
      "end time: 14:40:20\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[128256/8798814], loss:0.00128612\n",
      "end time: 14:40:32\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[130816/8798814], loss:0.00126671\n",
      "end time: 14:40:43\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[133376/8798814], loss:0.00125140\n",
      "end time: 14:40:54\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[135936/8798814], loss:0.00123002\n",
      "end time: 14:41:06\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], samples[138496/8798814], loss:0.00119879\n",
      "end time: 14:41:17\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[141056/8798814], loss:0.00118655\n",
      "end time: 14:41:28\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[143616/8798814], loss:0.00116409\n",
      "end time: 14:41:40\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[146176/8798814], loss:0.00114781\n",
      "end time: 14:41:51\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[148736/8798814], loss:0.00112310\n",
      "end time: 14:42:02\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[151296/8798814], loss:0.00110775\n",
      "end time: 14:42:14\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[153856/8798814], loss:0.00108825\n",
      "end time: 14:42:25\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[156416/8798814], loss:0.00107425\n",
      "end time: 14:42:36\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[158976/8798814], loss:0.00107005\n",
      "end time: 14:42:48\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[161536/8798814], loss:0.00104041\n",
      "end time: 14:42:59\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[164096/8798814], loss:0.00101982\n",
      "end time: 14:43:11\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[166656/8798814], loss:0.00100527\n",
      "end time: 14:43:22\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[169216/8798814], loss:0.00099631\n",
      "end time: 14:43:34\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[171776/8798814], loss:0.00096837\n",
      "end time: 14:43:46\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[174336/8798814], loss:0.00096427\n",
      "end time: 14:43:58\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[176896/8798814], loss:0.00094699\n",
      "end time: 14:44:10\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[179456/8798814], loss:0.00092143\n",
      "end time: 14:44:22\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[182016/8798814], loss:0.00091911\n",
      "end time: 14:44:34\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[184576/8798814], loss:0.00089359\n",
      "end time: 14:44:46\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[187136/8798814], loss:0.00088150\n",
      "end time: 14:44:57\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[189696/8798814], loss:0.00086347\n",
      "end time: 14:45:09\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[192256/8798814], loss:0.00084755\n",
      "end time: 14:45:21\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[194816/8798814], loss:0.00084748\n",
      "end time: 14:45:32\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[197376/8798814], loss:0.00082933\n",
      "end time: 14:45:44\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[199936/8798814], loss:0.00080882\n",
      "end time: 14:45:56\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[202496/8798814], loss:0.00081012\n",
      "end time: 14:46:08\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[205056/8798814], loss:0.00079225\n",
      "end time: 14:46:20\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[207616/8798814], loss:0.00078666\n",
      "end time: 14:46:31\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[210176/8798814], loss:0.00075685\n",
      "end time: 14:46:43\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[212736/8798814], loss:0.00075102\n",
      "end time: 14:46:55\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[215296/8798814], loss:0.00074143\n",
      "end time: 14:47:07\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[217856/8798814], loss:0.00073284\n",
      "end time: 14:47:19\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[220416/8798814], loss:0.00071210\n",
      "end time: 14:47:31\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[222976/8798814], loss:0.00070651\n",
      "end time: 14:47:43\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[225536/8798814], loss:0.00068536\n",
      "end time: 14:47:55\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[228096/8798814], loss:0.00067960\n",
      "end time: 14:48:06\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[230656/8798814], loss:0.00067963\n",
      "end time: 14:48:18\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[233216/8798814], loss:0.00066754\n",
      "end time: 14:48:30\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[235776/8798814], loss:0.00065468\n",
      "end time: 14:48:42\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[238336/8798814], loss:0.00063620\n",
      "end time: 14:48:53\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[240896/8798814], loss:0.00062967\n",
      "end time: 14:49:05\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[243456/8798814], loss:0.00061726\n",
      "end time: 14:49:17\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[246016/8798814], loss:0.00061391\n",
      "end time: 14:49:29\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[248576/8798814], loss:0.00060792\n",
      "end time: 14:49:41\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[251136/8798814], loss:0.00059729\n",
      "end time: 14:49:52\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[253696/8798814], loss:0.00059204\n",
      "end time: 14:50:04\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[256256/8798814], loss:0.00057439\n",
      "end time: 14:50:16\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[258816/8798814], loss:0.00057443\n",
      "end time: 14:50:27\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[261376/8798814], loss:0.00056251\n",
      "end time: 14:50:39\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[263936/8798814], loss:0.00054559\n",
      "end time: 14:50:51\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[266496/8798814], loss:0.00055230\n",
      "end time: 14:51:03\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[269056/8798814], loss:0.00054413\n",
      "end time: 14:51:15\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[271616/8798814], loss:0.00053185\n",
      "end time: 14:51:26\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[274176/8798814], loss:0.00051636\n",
      "end time: 14:51:38\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], samples[276736/8798814], loss:0.00052438\n",
      "end time: 14:51:50\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[279296/8798814], loss:0.00050361\n",
      "end time: 14:52:01\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[281856/8798814], loss:0.00049404\n",
      "end time: 14:52:13\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[284416/8798814], loss:0.00049019\n",
      "end time: 14:52:24\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[286976/8798814], loss:0.00048515\n",
      "end time: 14:52:36\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[289536/8798814], loss:0.00046965\n",
      "end time: 14:52:47\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[292096/8798814], loss:0.00045893\n",
      "end time: 14:52:58\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[294656/8798814], loss:0.00045473\n",
      "end time: 14:53:11\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[297216/8798814], loss:0.00045146\n",
      "end time: 14:53:23\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[299776/8798814], loss:0.00044739\n",
      "end time: 14:53:35\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[302336/8798814], loss:0.00043838\n",
      "end time: 14:53:47\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[304896/8798814], loss:0.00043709\n",
      "end time: 14:53:59\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[307456/8798814], loss:0.00043833\n",
      "end time: 14:54:11\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[310016/8798814], loss:0.00042449\n",
      "end time: 14:54:23\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[312576/8798814], loss:0.00041520\n",
      "end time: 14:54:34\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[315136/8798814], loss:0.00041653\n",
      "end time: 14:54:46\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[317696/8798814], loss:0.00040874\n",
      "end time: 14:54:58\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[320256/8798814], loss:0.00041683\n",
      "end time: 14:55:09\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[322816/8798814], loss:0.00039765\n",
      "end time: 14:55:21\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[325376/8798814], loss:0.00038876\n",
      "end time: 14:55:33\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[327936/8798814], loss:0.00038165\n",
      "end time: 14:55:45\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[330496/8798814], loss:0.00037763\n",
      "end time: 14:55:56\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[333056/8798814], loss:0.00038070\n",
      "end time: 14:56:08\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[335616/8798814], loss:0.00036667\n",
      "end time: 14:56:20\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[338176/8798814], loss:0.00036393\n",
      "end time: 14:56:31\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[340736/8798814], loss:0.00036336\n",
      "end time: 14:56:43\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[343296/8798814], loss:0.00035052\n",
      "end time: 14:56:55\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[345856/8798814], loss:0.00034153\n",
      "end time: 14:57:07\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[348416/8798814], loss:0.00035947\n",
      "end time: 14:57:19\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[350976/8798814], loss:0.00034147\n",
      "end time: 14:57:30\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[353536/8798814], loss:0.00034317\n",
      "end time: 14:57:42\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[356096/8798814], loss:0.00032522\n",
      "end time: 14:57:54\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[358656/8798814], loss:0.00033095\n",
      "end time: 14:58:06\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[361216/8798814], loss:0.00032842\n",
      "end time: 14:58:17\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[363776/8798814], loss:0.00031226\n",
      "end time: 14:58:29\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[366336/8798814], loss:0.00031610\n",
      "end time: 14:58:42\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[368896/8798814], loss:0.00031757\n",
      "end time: 14:58:54\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[371456/8798814], loss:0.00031135\n",
      "end time: 14:59:06\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[374016/8798814], loss:0.00030229\n",
      "end time: 14:59:18\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[376576/8798814], loss:0.00029533\n",
      "end time: 14:59:30\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[379136/8798814], loss:0.00029697\n",
      "end time: 14:59:42\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[381696/8798814], loss:0.00029905\n",
      "end time: 14:59:54\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[384256/8798814], loss:0.00028721\n",
      "end time: 15:00:06\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[386816/8798814], loss:0.00028760\n",
      "end time: 15:00:18\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[389376/8798814], loss:0.00029185\n",
      "end time: 15:00:30\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[391936/8798814], loss:0.00028588\n",
      "end time: 15:00:42\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[394496/8798814], loss:0.00027891\n",
      "end time: 15:00:53\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[397056/8798814], loss:0.00027198\n",
      "end time: 15:01:05\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[399616/8798814], loss:0.00027157\n",
      "end time: 15:01:17\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[402176/8798814], loss:0.00026197\n",
      "end time: 15:01:29\n",
      "--------------------------------------------------------------------------------\n",
      "epoch [1/1], samples[404736/8798814], loss:0.00026710\n",
      "end time: 15:01:41\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-485e153762b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# step: batch index; x.shape: [BATCH_SIZE, 1, 374251]; y.shape: [BATCH_SIZE]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mb_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# batch x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mb_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# batch label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader): # step: batch index; x.shape: [BATCH_SIZE, 1, 374251]; y.shape: [BATCH_SIZE]   \n",
    "        b_x = torch.tensor(x).to(device)                   # batch x\n",
    "        b_y = torch.tensor(x).to(device)\n",
    "        b_label = torch.tensor(y).to(device)               # batch label\n",
    "\n",
    "        encoded, decoded = autoencoder(b_x)\n",
    "\n",
    "        loss = criterion(decoded, b_y)      # mean square error\n",
    "        optimizer.zero_grad()               # clear gradients for this training step since gradients are accumulated in the process\n",
    "        loss.backward()                     # backpropagation, compute gradients\n",
    "        optimizer.step()                    # apply gradients. \"The function can be called once the gradients are computed using eg. backward()\"\n",
    "\n",
    "        # Currently the training speed is really slow, change the numbers below according to what you need\n",
    "        if step % 10 == 0:\n",
    "            print('epoch [{}/{}], samples[{}/{}], loss:{:.8f}'\n",
    "                  .format(epoch + 1, EPOCH, (step + 1) * BATCH_SIZE, train_X.shape[0], loss.item()))\n",
    "            print('end time: {}'.format(pu.get_time_str()))\n",
    "            print('-' * 80)\n",
    "#         if step == 100:\n",
    "#             break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input = train_dataset.input[0]\n",
    "first_data = csr_to_tensor(first_input, torch.Size([1,first_input.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data, _ = autoencoder(first_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
