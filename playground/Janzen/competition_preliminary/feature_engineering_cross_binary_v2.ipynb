{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../../code/utils')\n",
    "sys.path.append('../../../code/pipeline')\n",
    "sys.path.append('../../../code')\n",
    "import data_utils as du\n",
    "import perf_utils as pu\n",
    "import data_jointer as dj\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_list(dic):\n",
    "    \"\"\"Given a dictionary mapping something to integers, return list of keys sorted by their values\"\"\"\n",
    "    return [k for k, v in sorted(dic.items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_vectorize(matrix, selectors, col_names, prefixes=None):\n",
    "    \"\"\"Construct cross binary matrix from a matrix, a selector array (most likely the ad feature values in this case)\"\"\"\n",
    "    assert matrix.shape[0] == selectors.shape[0]\n",
    "    num_rows = matrix.shape[0]\n",
    "    num_cols = matrix.shape[1]  # get count of columns for quick crossing\n",
    "    \n",
    "    # preparation for selectors\n",
    "    selectors = pd.Series(selectors) if not isinstance(selectors, pd.Series) else selectors\n",
    "    unique_selectors = np.unique(selectors.values)  # get unique selector values for crossing\n",
    "    selector_to_offset = {selector: i * num_cols for i, selector in enumerate(unique_selectors)}  # map unique selector to offset\n",
    "    \n",
    "    # preparation for each row\n",
    "    row_lengths = np.squeeze(np.asarray(matrix.sum(axis=1)))  # get number of non-zeros in each row for quick crossing\n",
    "    row_offsets = selectors.map(selector_to_offset).values  # get offset of each row in the cross matrix\n",
    "    offsets = np.repeat(row_offsets, row_lengths)  # get offsets for matrix.indices for quick crossing\n",
    "    \n",
    "    # construct cross matrix\n",
    "    cross_indices = offsets + matrix.indices  # calculate the indices for the cross matrix\n",
    "    cross_matrix = sparse.csr_matrix((matrix.data, cross_indices, matrix.indptr), \n",
    "                                     shape=(num_rows, num_cols * len(unique_selectors)),\n",
    "                                     dtype=np.int8)  # construct the cross matrix\n",
    "    \n",
    "    # get column names for the cross matrix\n",
    "    cross_col_names = []\n",
    "    if prefixes is None:\n",
    "        for selector in unique_selectors:\n",
    "            cross_col_names += [\"{}x{}\".format(selector, col_name) for col_name in col_names]\n",
    "    else:\n",
    "        prefix1, prefix2 = prefixes  # \n",
    "        for selector in unique_selectors:\n",
    "            cross_col_names += [\"{}_{}_x_{}_{}\".format(prefix1, selector, prefix2, col_name) \n",
    "                                for col_name in col_names]\n",
    "            \n",
    "    assert cross_matrix.shape[1] == len(cross_col_names)\n",
    "    return cross_matrix, cross_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossbin_folder = os.path.join(config.PRELIM_NLP_COUNT_DATA_DIR, \"simple_cross/byUserFeatureName\")\n",
    "\n",
    "def cross_binary_path(ad_feat_name, user_feat_name, prefix=\"train\", create=True):\n",
    "    folder = os.path.join(crossbin_folder, \"[featureName='{}']\".format(user_feat_name))\n",
    "    file = \"{}.[adFeatureName='{}'].binary.pkl\".format(prefix, ad_feat_name)\n",
    "    path = os.path.join(folder, file)\n",
    "    if create:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [('productId', 'LBS'),\n",
    "         ('advertiserId', 'interest1'),\n",
    "         ('aid', 'interest2'),\n",
    "         ('creativeSize', 'interest2'), \n",
    "         ('campaignId', 'interest4'),  # whether to keep it? \n",
    "         ('aid', 'interest5'),  \n",
    "         ('productType', 'kw1'),  # 'kw1' looks very overfitting prone, to be decide whether to keep it\n",
    "         ('productType', 'kw2'),\n",
    "         ('productType', 'kw3'),\n",
    "         ('productType', 'topic1'),\n",
    "         ('aid', 'topic2'),\n",
    "         ('productType', 'topic2'),\n",
    "         # ('productType', 'topic3'),  # might help in predicting negative samples\n",
    "         # ('productType', 'appIdInstall'),  # might help in predicting negative samples\n",
    "         # ('productType', 'appIdAction'),  # might help in predicting negative samples\n",
    "         ('aid', 'ct'),\n",
    "         ('aid', 'os')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufeat_to_afeats = {}\n",
    "for afeat, ufeat in pairs:\n",
    "    if ufeat in ufeat_to_afeats:\n",
    "        ufeat_to_afeats[ufeat] += [afeat]\n",
    "    else:\n",
    "        ufeat_to_afeats[ufeat] = [afeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 8798814\n",
      "Test Size: 2265989\n",
      "Concatenated Data Shape: (11064803, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train = du.load_raw_data(\"train\")\n",
    "df_test = du.load_raw_data(\"test\")\n",
    "df_ad = du.load_raw_data(\"ad\")\n",
    "train_size = df_train.shape[0]\n",
    "test_size = df_test.shape[0]\n",
    "\n",
    "df_all = pd.concat([df_train, df_test], ignore_index=True)  # concatenate DataFrames and then split at the end; to speed up\n",
    "print(\"Train Size: {}\".format(train_size))\n",
    "print(\"Test Size: {}\".format(test_size))\n",
    "print(\"Concatenated Data Shape: {}\".format(df_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:43:51] Finish joining train+test DataFrame and ad DataFrame. △M: +675.63MB. △T: 2.4 seconds.\n"
     ]
    }
   ],
   "source": [
    "# define jointer\n",
    "ad_jointer = dj.PandasPandasJointer(\"aid\")\n",
    "user_jointer = dj.PandasMatrixJointer(\"uid\")\n",
    "\n",
    "with pu.profiler(\"joining train+test DataFrame and ad DataFrame\"):\n",
    "    df_all = ad_jointer.join(df1=df_all, df2=df_ad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:44:20] Finish vectorizing and saving 'kw3'x'productType' binary. △M: -330.05MB. △T: 2.7 seconds.\n",
      "[03:45:03] Finish vectorizing and saving 'interest1'x'advertiserId' binary. △M: -278.36MB. △T: 11.4 seconds.\n",
      "[03:45:37] Finish vectorizing and saving 'kw1'x'productType' binary. △M: -378.86MB. △T: 6.1 seconds.\n",
      "[03:46:11] Finish vectorizing and saving 'kw2'x'productType' binary. △M: -383.86MB. △T: 5.1 seconds.\n",
      "[03:46:40] Finish vectorizing and saving 'interest4'x'campaignId' binary. △M: -304.98MB. △T: 3.3 seconds.\n",
      "[03:47:12] Finish vectorizing and saving 'interest2'x'aid' binary. △M: -384.79MB. △T: 5.9 seconds.\n",
      "[03:47:17] Finish vectorizing and saving 'interest2'x'creativeSize' binary. △M: +48.71MB. △T: 4.8 seconds.\n",
      "[03:47:47] Finish vectorizing and saving 'ct'x'aid' binary. △M: -288.03MB. △T: 4.8 seconds.\n",
      "[03:48:17] Finish vectorizing and saving 'os'x'aid' binary. △M: -303.93MB. △T: 3.6 seconds.\n",
      "[03:48:50] Finish vectorizing and saving 'topic1'x'productType' binary. △M: -382.7MB. △T: 4.9 seconds.\n",
      "[03:49:34] Finish vectorizing and saving 'interest5'x'aid' binary. △M: -382.61MB. △T: 12.1 seconds.\n",
      "[03:50:10] Finish vectorizing and saving 'topic2'x'aid' binary. △M: -369.85MB. △T: 8.5 seconds.\n",
      "[03:50:15] Finish vectorizing and saving 'topic2'x'productType' binary. △M: +41.16MB. △T: 5.0 seconds.\n",
      "[03:50:46] Finish vectorizing and saving 'LBS'x'productId' binary. △M: -298.89MB. △T: 3.1 seconds.\n"
     ]
    }
   ],
   "source": [
    "for ufeat, afeats in ufeat_to_afeats.items():\n",
    "    ### given a user feature ###\n",
    "    # load and join user matrix\n",
    "    row_uids, (word_to_index, user_matrix) = du.load_user_cnt(ufeat)\n",
    "    col_names = dict_to_list(word_to_index)\n",
    "    matrix = user_jointer.join(df=df_all, matrix=user_matrix, row_names=row_uids)\n",
    "    \n",
    "    for afeat in afeats:\n",
    "        ### given a ad feature ###\n",
    "        # construct cross matrix\n",
    "        with pu.profiler(\"vectorizing and saving '{}'x'{}' binary\".format(ufeat, afeat)):\n",
    "            cross_matrix, cross_col_names = cross_vectorize(matrix, df_all[afeat], col_names, [afeat, ufeat])\n",
    "            \n",
    "             # save train matrix\n",
    "            out_path = cross_binary_path(afeat, ufeat, prefix=\"train\")\n",
    "            cross_matrix_train = cross_matrix[:train_size, :]\n",
    "            du.save_pickle((cross_col_names, cross_matrix_train), out_path)\n",
    "            del cross_matrix_train\n",
    "            gc.collect()\n",
    "\n",
    "            # save test matrix\n",
    "            out_path = cross_binary_path(afeat, ufeat, prefix=\"test1\")\n",
    "            cross_matrix_test = cross_matrix[train_size:, :]\n",
    "            du.save_pickle((cross_col_names, cross_matrix_test), out_path)\n",
    "            del cross_matrix_test\n",
    "            gc.collect()\n",
    "\n",
    "            # release memory and collect garbage\n",
    "            del cross_matrix\n",
    "            del cross_col_names\n",
    "            gc.collect()\n",
    "            \n",
    "    # release memory and collect garbage\n",
    "    del user_matrix\n",
    "    del matrix\n",
    "    del col_names\n",
    "    del word_to_index\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
