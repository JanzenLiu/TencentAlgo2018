{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from contextlib import contextmanager\n",
    "import sys\n",
    "sys.path.append('../../../code/utils')\n",
    "from perf_utils import get_memory_str, get_memory_bytes, format_memory_diff, format_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURE_DIR = '../../../data/split/preliminary_contest_data/byUserFeatureName/'\n",
    "VOCAB_DIR = '../../../data/vocabulary/preliminary_contest_data/'\n",
    "\n",
    "\n",
    "def feature_path(feat_name):\n",
    "    filename = \"userFeature.[featureName='{}'].data\".format(feat_name)\n",
    "    return os.path.join(FEATURE_DIR, filename)\n",
    "\n",
    "\n",
    "def vocab_path(feat_name=\"all\"):\n",
    "    if feat_name == \"all\":\n",
    "        filename = \"userFeature.pkl\"\n",
    "    else:\n",
    "        filename = \"userFeature.[featureName='{}'].pkl\".format(feat_name)\n",
    "    return os.path.join(VOCAB_DIR, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    obj = None\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def save_as_pickle(obj, filepath):\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_feature(feat_name, **kw):\n",
    "    sep = kw.pop('sep', '|')\n",
    "    dtype = kw.pop('dtype', {feat_name: str})\n",
    "    filepath = feature_path(feat_name)\n",
    "    return pd.read_csv(filepath, sep=sep, dtype=dtype, **kw)\n",
    "\n",
    "def load_vocab(feat_name='all'):\n",
    "    filepath = vocab_path(feat_name)\n",
    "    return load_pickle(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_str():\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def profiler(task_name, verbose_memory=True, verbose_time=True):\n",
    "    t0 = time.time()\n",
    "    m0 = get_memory_bytes()\n",
    "    yield\n",
    "    t_delta = time.time() - t0\n",
    "    m_delta = get_memory_bytes() - m0\n",
    "    msg = \"[{}] Finish {}.\".format(get_time_str(), task_name)\n",
    "    if verbose_memory:\n",
    "        msg += \" △M: {}.\".format(format_memory_diff(m_delta))\n",
    "    if verbose_time:\n",
    "        msg += \" △T: {}.\".format(format_secs(t_delta))\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_vectorize(series, vocab):\n",
    "    # My implementation of CountVectorizer FOR THIS CASE ONLY\n",
    "    # see https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "    # for the standard CSR representation.\n",
    "    # It's faster since:\n",
    "    # 1. There is no checking in each iteration, because I assume that the input text and vocabulary matches\n",
    "    # 2. Technically I didn't use iteration, I use pandas apply instead. That's why the input text must be pd.Series\n",
    "    # 3. I use library like `itertools` instead of starting from scratch by myself\n",
    "    # 4. It will be much faster theoretically if I add multiprocessing\n",
    "    vocab_map = {val: i for i, val in enumerate(vocab)}  # mapping word(str) to column index(int)\n",
    "    lst_series = series.apply(lambda x: [vocab_map[val] for val in x.split()])  # pd.Series with each row: list of int\n",
    "    cnt_series = lst_series.apply(len)  # pd.Series with each row: int, indicating the number of words\n",
    "    indptr = np.concatenate((np.zeros(1), np.add.accumulate(cnt_series)))  # there should be a zeros at the beginning\n",
    "    indices = list(itertools.chain.from_iterable(lst_series))  # to concatenate lists from all rows\n",
    "    data = np.ones(len(indices), dtype=np.int8)  # all non-zero value is one.\n",
    "    cnt_vec = csr_matrix((data, indices, indptr), dtype=np.int8)  # see the link above for detailed explanation\n",
    "    \n",
    "    # clean memory. I am not sure whether they work. to be checked\n",
    "    vocab_map.clear()\n",
    "    del [[lst_series, cnt_series]]\n",
    "    del indptr\n",
    "    del indices\n",
    "    del data\n",
    "    gc.collect()\n",
    "    lst_series = pd.DataFrame()  # not sure whether it work\n",
    "    cnt_series = pd.DataFrame()  # same\n",
    "\n",
    "    return cnt_vec, vocab_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt_dir = '../../../data/nlp_count/preliminary_contest_data/byUserFeatureName/'\n",
    "tfidf_dir = '../../../data/nlp_tfidf/preliminary_contest_data/byUserFeatureName/'\n",
    "os.makedirs(cnt_dir, exist_ok=True)\n",
    "os.makedirs(tfidf_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage at this moment: 105.88MB\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'age'...\n",
      "[16:27:58] Finish loading and preprocessing. △M: +148.58MB. △T: 2.2 seconds.\n",
      "[16:28:22] Finish count vectorizing (ngram=1) [sklearn]. △M: +83.95MB. △T: 24.8 seconds.\n",
      "[16:28:35] Finish count vectorizing (ngram=1) [Janzen's]. △M: +157.99MB. △T: 12.5 seconds.\n",
      "[16:28:35] Finish saving count vectors. △M: -240.17MB.\n",
      "[16:29:00] Finish TFIDF vectorizing (ngram=1) transformation. △M: +148.05MB. △T: 24.9 seconds.\n",
      "[16:29:01] Finish saving TFIDF vectors. △M: -147.8MB.\n",
      "[16:29:01] Finish cleaning. △M: -147.81MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'gender'...\n",
      "[16:29:03] Finish loading and preprocessing. △M: +146.83MB. △T: 2.0 seconds.\n",
      "[16:29:28] Finish count vectorizing (ngram=1) [sklearn]. △M: +83.16MB. △T: 24.6 seconds.\n",
      "[16:29:40] Finish count vectorizing (ngram=1) [Janzen's]. △M: +158.05MB. △T: 12.4 seconds.\n",
      "[16:29:40] Finish saving count vectors. △M: -240.7MB.\n",
      "[16:30:05] Finish TFIDF vectorizing (ngram=1) transformation. △M: +148.67MB. △T: 24.9 seconds.\n",
      "[16:30:06] Finish saving TFIDF vectors. △M: -147.81MB.\n",
      "[16:30:06] Finish cleaning. △M: -147.81MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'marriageStatus'...\n",
      "[16:30:08] Finish loading and preprocessing. △M: +146.71MB. △T: 2.3 seconds.\n",
      "[16:30:34] Finish count vectorizing (ngram=1) [sklearn]. △M: +92.71MB. △T: 25.5 seconds.\n",
      "[16:30:47] Finish count vectorizing (ngram=1) [Janzen's]. △M: +174.83MB. △T: 13.2 seconds.\n",
      "[16:30:47] Finish saving count vectors. △M: -265.66MB.\n",
      "[16:31:13] Finish TFIDF vectorizing (ngram=1) transformation. △M: +168.27MB. △T: 26.0 seconds.\n",
      "[16:31:14] Finish saving TFIDF vectors. △M: -169.21MB.\n",
      "[16:31:14] Finish cleaning. △M: -147.81MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'education'...\n",
      "[16:31:16] Finish loading and preprocessing. △M: +147.79MB. △T: 2.1 seconds.\n",
      "[16:31:41] Finish count vectorizing (ngram=1) [sklearn]. △M: +83.8MB. △T: 24.6 seconds.\n",
      "[16:31:53] Finish count vectorizing (ngram=1) [Janzen's]. △M: +157.08MB. △T: 12.3 seconds.\n",
      "[16:31:53] Finish saving count vectors. △M: -240.45MB.\n",
      "[16:32:19] Finish TFIDF vectorizing (ngram=1) transformation. △M: +148.78MB. △T: 25.3 seconds.\n",
      "[16:32:19] Finish saving TFIDF vectors. △M: -147.82MB.\n",
      "[16:32:19] Finish cleaning. △M: -147.81MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'consumptionAbility'...\n",
      "[16:32:21] Finish loading and preprocessing. △M: +145.82MB. △T: 2.1 seconds.\n",
      "[16:32:46] Finish count vectorizing (ngram=1) [sklearn]. △M: +83.73MB. △T: 24.6 seconds.\n",
      "[16:32:59] Finish count vectorizing (ngram=1) [Janzen's]. △M: +150.5MB. △T: 12.5 seconds.\n",
      "[16:32:59] Finish saving count vectors. △M: -239.46MB.\n",
      "[16:33:24] Finish TFIDF vectorizing (ngram=1) transformation. △M: +149.42MB. △T: 25.0 seconds.\n",
      "[16:33:24] Finish saving TFIDF vectors. △M: -147.81MB.\n",
      "[16:33:25] Finish cleaning. △M: -148.06MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'LBS'...\n",
      "[16:33:27] Finish loading and preprocessing. △M: +221.1MB. △T: 2.7 seconds.\n",
      "[16:33:52] Finish count vectorizing (ngram=1) [sklearn]. △M: +82.93MB. △T: 24.9 seconds.\n",
      "[16:34:05] Finish count vectorizing (ngram=1) [Janzen's]. △M: +160.05MB. △T: 12.6 seconds.\n",
      "[16:34:05] Finish saving count vectors. △M: -240.45MB.\n",
      "[16:34:30] Finish TFIDF vectorizing (ngram=1) transformation. △M: +146.0MB. △T: 25.3 seconds.\n",
      "[16:34:31] Finish saving TFIDF vectors. △M: -147.74MB.\n",
      "[16:34:31] Finish cleaning. △M: -222.22MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest1'...\n",
      "[16:34:40] Finish loading and preprocessing. △M: +897.24MB. △T: 9.1 seconds.\n",
      "[16:35:57] Finish count vectorizing (ngram=1) [sklearn]. △M: +646.65MB. △T: 1.3 minutes.\n",
      "[16:36:48] Finish count vectorizing (ngram=1) [Janzen's]. △M: +949.68MB. △T: 50.8 seconds.\n",
      "[16:36:50] Finish saving count vectors. △M: -1.77GB.\n",
      "[16:38:13] Finish TFIDF vectorizing (ngram=1) transformation. △M: +1.65GB. △T: 1.4 minutes.\n",
      "[16:38:18] Finish saving TFIDF vectors. △M: -1.46GB.\n",
      "[16:38:19] Finish cleaning. △M: -893.21MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest2'...\n",
      "[16:38:24] Finish loading and preprocessing. △M: +394.14MB. △T: 5.1 seconds.\n",
      "[16:39:10] Finish count vectorizing (ngram=1) [sklearn]. △M: +223.98MB. △T: 46.3 seconds.\n",
      "[16:39:37] Finish count vectorizing (ngram=1) [Janzen's]. △M: +412.51MB. △T: 27.2 seconds.\n",
      "[16:39:38] Finish saving count vectors. △M: -634.86MB.\n",
      "[16:40:23] Finish TFIDF vectorizing (ngram=1) transformation. △M: +484.74MB. △T: 45.1 seconds.\n",
      "[16:40:25] Finish saving TFIDF vectors. △M: -485.66MB.\n",
      "[16:40:25] Finish cleaning. △M: -393.72MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest3'...\n",
      "[16:40:28] Finish loading and preprocessing. △M: +221.0MB. △T: 2.5 seconds.\n",
      "[16:40:55] Finish count vectorizing (ngram=1) [sklearn]. △M: +91.27MB. △T: 27.3 seconds.\n",
      "[16:41:10] Finish count vectorizing (ngram=1) [Janzen's]. △M: +172.66MB. △T: 15.1 seconds.\n",
      "[16:41:11] Finish saving count vectors. △M: -262.26MB.\n",
      "[16:41:38] Finish TFIDF vectorizing (ngram=1) transformation. △M: +165.37MB. △T: 27.4 seconds.\n",
      "[16:41:39] Finish saving TFIDF vectors. △M: -166.29MB.\n",
      "[16:41:39] Finish cleaning. △M: -223.97MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest4'...\n",
      "[16:41:41] Finish loading and preprocessing. △M: +221.68MB. △T: 2.3 seconds.\n",
      "[16:42:09] Finish count vectorizing (ngram=1) [sklearn]. △M: +85.93MB. △T: 27.8 seconds.\n",
      "[16:42:23] Finish count vectorizing (ngram=1) [Janzen's]. △M: +161.99MB. △T: 14.1 seconds.\n",
      "[16:42:24] Finish saving count vectors. △M: -247.2MB.\n",
      "[16:42:51] Finish TFIDF vectorizing (ngram=1) transformation. △M: +154.5MB. △T: 27.4 seconds.\n",
      "[16:42:52] Finish saving TFIDF vectors. △M: -153.39MB.\n",
      "[16:42:52] Finish cleaning. △M: -221.97MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'interest5'...\n",
      "[16:43:04] Finish loading and preprocessing. △M: +993.11MB. △T: 12.2 seconds.\n",
      "[16:44:43] Finish count vectorizing (ngram=1) [sklearn]. △M: +735.75MB. △T: 1.6 minutes.\n",
      "[16:45:55] Finish count vectorizing (ngram=1) [Janzen's]. △M: +536.1MB. △T: 1.2 minutes.\n",
      "[16:45:59] Finish saving count vectors. △M: -1.39GB.\n",
      "[16:47:44] Finish TFIDF vectorizing (ngram=1) transformation. △M: +1.67GB. △T: 1.7 minutes.\n",
      "[16:47:51] Finish saving TFIDF vectors. △M: -1.72GB.\n",
      "[16:47:52] Finish cleaning. △M: -791.78MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'kw1'...\n",
      "[16:48:04] Finish loading and preprocessing. △M: +953.19MB. △T: 11.8 seconds.\n",
      "[16:49:09] Finish count vectorizing (ngram=1) [sklearn]. △M: +261.11MB. △T: 1.1 minutes.\n",
      "[16:50:00] Finish count vectorizing (ngram=1) [Janzen's]. △M: +442.41MB. △T: 51.6 seconds.\n",
      "[16:50:01] Finish saving count vectors. △M: -700.27MB.\n",
      "[16:51:10] Finish TFIDF vectorizing (ngram=1) transformation. △M: +543.98MB. △T: 1.1 minutes.\n",
      "[16:51:17] Finish saving TFIDF vectors. △M: -547.88MB.\n",
      "[16:51:18] Finish cleaning. △M: -952.2MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'kw2'...\n",
      "[16:51:30] Finish loading and preprocessing. △M: +888.32MB. △T: 11.9 seconds.\n",
      "[16:52:30] Finish count vectorizing (ngram=1) [sklearn]. △M: +262.91MB. △T: 59.8 seconds.\n",
      "[16:53:10] Finish count vectorizing (ngram=1) [Janzen's]. △M: +472.3MB. △T: 40.6 seconds.\n",
      "[16:53:11] Finish saving count vectors. △M: -733.07MB.\n",
      "[16:54:12] Finish TFIDF vectorizing (ngram=1) transformation. △M: +569.88MB. △T: 1.0 minutes.\n",
      "[16:54:14] Finish saving TFIDF vectors. △M: -573.04MB.\n",
      "[16:54:15] Finish cleaning. △M: -887.47MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'kw3'...\n",
      "[16:54:18] Finish loading and preprocessing. △M: +250.89MB. △T: 2.9 seconds.\n",
      "[16:54:48] Finish count vectorizing (ngram=1) [sklearn]. △M: +93.44MB. △T: 30.1 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:55:04] Finish count vectorizing (ngram=1) [Janzen's]. △M: +172.94MB. △T: 16.4 seconds.\n",
      "[16:55:05] Finish saving count vectors. △M: -267.18MB.\n",
      "[16:55:36] Finish TFIDF vectorizing (ngram=1) transformation. △M: +171.58MB. △T: 31.4 seconds.\n",
      "[16:55:37] Finish saving TFIDF vectors. △M: -171.57MB.\n",
      "[16:55:37] Finish cleaning. △M: -250.97MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'topic1'...\n",
      "[16:55:47] Finish loading and preprocessing. △M: +878.09MB. △T: 9.9 seconds.\n",
      "[16:56:35] Finish count vectorizing (ngram=1) [sklearn]. △M: +254.57MB. △T: 48.6 seconds.\n",
      "[16:57:06] Finish count vectorizing (ngram=1) [Janzen's]. △M: +463.91MB. △T: 30.5 seconds.\n",
      "[16:57:07] Finish saving count vectors. △M: -716.78MB.\n",
      "[16:57:53] Finish TFIDF vectorizing (ngram=1) transformation. △M: +555.59MB. △T: 46.2 seconds.\n",
      "[16:57:55] Finish saving TFIDF vectors. △M: -555.46MB.\n",
      "[16:57:55] Finish cleaning. △M: -876.47MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'topic2'...\n",
      "[16:58:02] Finish loading and preprocessing. △M: +714.66MB. △T: 7.2 seconds.\n",
      "[16:58:53] Finish count vectorizing (ngram=1) [sklearn]. △M: +262.16MB. △T: 50.2 seconds.\n",
      "[16:59:25] Finish count vectorizing (ngram=1) [Janzen's]. △M: +477.81MB. △T: 32.5 seconds.\n",
      "[16:59:26] Finish saving count vectors. △M: -738.34MB.\n",
      "[17:00:13] Finish TFIDF vectorizing (ngram=1) transformation. △M: +574.21MB. △T: 47.2 seconds.\n",
      "[17:00:15] Finish saving TFIDF vectors. △M: -574.21MB.\n",
      "[17:00:16] Finish cleaning. △M: -714.97MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'topic3'...\n",
      "[17:00:18] Finish loading and preprocessing. △M: +245.43MB. △T: 2.5 seconds.\n",
      "[17:00:45] Finish count vectorizing (ngram=1) [sklearn]. △M: +96.08MB. △T: 26.5 seconds.\n",
      "[17:00:58] Finish count vectorizing (ngram=1) [Janzen's]. △M: +173.25MB. △T: 13.7 seconds.\n",
      "[17:00:59] Finish saving count vectors. △M: -268.8MB.\n",
      "[17:01:26] Finish TFIDF vectorizing (ngram=1) transformation. △M: +172.55MB. △T: 27.8 seconds.\n",
      "[17:01:27] Finish saving TFIDF vectors. △M: -172.54MB.\n",
      "[17:01:27] Finish cleaning. △M: -251.22MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'appIdInstall'...\n",
      "[17:01:31] Finish loading and preprocessing. △M: +368.34MB. △T: 3.9 seconds.\n",
      "[17:02:11] Finish count vectorizing (ngram=1) [sklearn]. △M: +195.52MB. △T: 40.3 seconds.\n",
      "[17:02:36] Finish count vectorizing (ngram=1) [Janzen's]. △M: +349.66MB. △T: 24.4 seconds.\n",
      "[17:02:36] Finish saving count vectors. △M: -539.62MB.\n",
      "[17:03:18] Finish TFIDF vectorizing (ngram=1) transformation. △M: +410.33MB. △T: 42.1 seconds.\n",
      "[17:03:20] Finish saving TFIDF vectors. △M: -404.86MB.\n",
      "[17:03:20] Finish cleaning. △M: -371.75MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'appIdAction'...\n",
      "[17:03:23] Finish loading and preprocessing. △M: +232.12MB. △T: 2.5 seconds.\n",
      "[17:03:49] Finish count vectorizing (ngram=1) [sklearn]. △M: +100.42MB. △T: 26.7 seconds.\n",
      "[17:04:03] Finish count vectorizing (ngram=1) [Janzen's]. △M: +169.83MB. △T: 13.5 seconds.\n",
      "[17:04:03] Finish saving count vectors. △M: -272.11MB.\n",
      "[17:04:30] Finish TFIDF vectorizing (ngram=1) transformation. △M: +173.61MB. △T: 27.0 seconds.\n",
      "[17:04:31] Finish saving TFIDF vectors. △M: -175.0MB.\n",
      "[17:04:31] Finish cleaning. △M: -231.47MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'ct'...\n",
      "[17:04:33] Finish loading and preprocessing. △M: +149.94MB. △T: 2.4 seconds.\n",
      "[17:05:02] Finish count vectorizing (ngram=1) [sklearn]. △M: +136.63MB. △T: 29.2 seconds.\n",
      "[17:05:19] Finish count vectorizing (ngram=1) [Janzen's]. △M: +235.36MB. △T: 16.1 seconds.\n",
      "[17:05:19] Finish saving count vectors. △M: -359.03MB.\n",
      "[17:05:49] Finish TFIDF vectorizing (ngram=1) transformation. △M: +249.0MB. △T: 30.3 seconds.\n",
      "[17:05:50] Finish saving TFIDF vectors. △M: -249.5MB.\n",
      "[17:05:50] Finish cleaning. △M: -147.81MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'os'...\n",
      "[17:05:53] Finish loading and preprocessing. △M: +147.75MB. △T: 2.3 seconds.\n",
      "[17:06:19] Finish count vectorizing (ngram=1) [sklearn]. △M: +70.58MB. △T: 26.5 seconds.\n",
      "[17:06:32] Finish count vectorizing (ngram=1) [Janzen's]. △M: +171.62MB. △T: 13.2 seconds.\n",
      "[17:06:33] Finish saving count vectors. △M: -250.12MB.\n",
      "[17:07:00] Finish TFIDF vectorizing (ngram=1) transformation. △M: +148.7MB. △T: 27.5 seconds.\n",
      "[17:07:01] Finish saving TFIDF vectors. △M: -156.32MB.\n",
      "[17:07:01] Finish cleaning. △M: -148.31MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'carrier'...\n",
      "[17:07:03] Finish loading and preprocessing. △M: +150.42MB. △T: 2.4 seconds.\n",
      "[17:07:30] Finish count vectorizing (ngram=1) [sklearn]. △M: +80.27MB. △T: 26.5 seconds.\n",
      "[17:07:43] Finish count vectorizing (ngram=1) [Janzen's]. △M: +165.24MB. △T: 13.4 seconds.\n",
      "[17:07:44] Finish saving count vectors. △M: -240.2MB.\n",
      "[17:08:10] Finish TFIDF vectorizing (ngram=1) transformation. △M: +140.14MB. △T: 26.5 seconds.\n",
      "[17:08:11] Finish saving TFIDF vectors. △M: -147.81MB.\n",
      "[17:08:11] Finish cleaning. △M: -148.06MB.\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 'house'...\n",
      "[17:08:13] Finish loading and preprocessing. △M: +224.33MB. △T: 2.5 seconds.\n",
      "[17:08:39] Finish count vectorizing (ngram=1) [sklearn]. △M: +80.26MB. △T: 26.1 seconds.\n",
      "[17:08:53] Finish count vectorizing (ngram=1) [Janzen's]. △M: +165.48MB. △T: 13.2 seconds.\n",
      "[17:08:53] Finish saving count vectors. △M: -240.45MB.\n",
      "[17:09:19] Finish TFIDF vectorizing (ngram=1) transformation. △M: +140.26MB. △T: 26.4 seconds.\n",
      "[17:09:20] Finish saving TFIDF vectors. △M: -147.81MB.\n",
      "[17:09:20] Finish cleaning. △M: -221.97MB.\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(string):\n",
    "    return string.split()\n",
    "\n",
    "\n",
    "feat_names = [\"age\", \"gender\", \"marriageStatus\", \"education\", \"consumptionAbility\", \"LBS\",\n",
    "              \"interest1\", \"interest2\", \"interest3\", \"interest4\", \"interest5\",\n",
    "              \"kw1\", \"kw2\", \"kw3\", \"topic1\", \"topic2\", \"topic3\", \"appIdInstall\",\n",
    "              \"appIdAction\", \"ct\", \"os\", \"carrier\", \"house\"]\n",
    "\n",
    "print(\"Memory usage at this moment: {}\".format(get_memory_str()))\n",
    "for i, feat_name in enumerate(feat_names):\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Processing '{}'...\".format(feat_name))\n",
    "    with profiler(\"loading and preprocessing\"):\n",
    "        df = load_feature(feat_name)  # pd.DataFrame\n",
    "        vocab = load_vocab(feat_name)  # list\n",
    "        docs = df[feat_name]  # pd.Series\n",
    "        if docs.isnull().sum() > 0:\n",
    "            fill_value = \"[nan]\"  # don't use [NaN]; fxxk sklearn\n",
    "            vocab += [fill_value]\n",
    "            docs = docs.fillna(fill_value)\n",
    "        \n",
    "    with profiler(\"count vectorizing (ngram=1) [sklearn]\"):\n",
    "        cnt_vectorizer = CountVectorizer(vocabulary=vocab, \n",
    "                                         tokenizer=tokenizer,\n",
    "                                         dtype=np.int8)\n",
    "        cnt_vec_sk = cnt_vectorizer.fit_transform(docs)\n",
    "        checksum_sk = cnt_vec_sk.getnnz()\n",
    "        \n",
    "    with profiler(\"count vectorizing (ngram=1) [Janzen's]\"):\n",
    "        # My method is about 60% faster than sklearn (though it is only applicable for this case)\n",
    "        # Just forget the memory usage, Jupyter's memory mechanism sucks. : )\n",
    "        # There must be some dirty py deal between sklearn/scipy/numpy/pandas and Jupyter, fxxk them all\n",
    "        # I am faster and I haven't add multiprocessing and cython yet. : )\n",
    "        # I believe if I add 4-way multiprocessing, the speed will be at least doubled compared with this version\n",
    "        # Fxxk Jupyter Notebook\n",
    "        cnt_vec_jz, cnt_dict = count_vectorize(docs, vocab)\n",
    "        checksum_jz = cnt_vec_jz.getnnz()\n",
    "        \n",
    "    assert checksum_jz == checksum_sk\n",
    "    \n",
    "    with profiler(\"saving count vectors\", verbose_time=False):\n",
    "        cnt_file = \"userFeature.[featureName='{}'].pkl\".format(feat_name)\n",
    "        cnt_path = os.path.join(cnt_dir, cnt_file)\n",
    "        # save_as_pickle((cnt_dict, cnt_vec_jz), cnt_path)\n",
    "        save_as_pickle((cnt_vectorizer.vocabulary_, cnt_vec_sk), cnt_path) # save mapping as well for further analysis\n",
    "        if i == 0:\n",
    "            uid_file = \"uid.pkl\"\n",
    "            uid_path = os.path.join(cnt_dir, uid_file)\n",
    "            save_as_pickle(df['uid'].values, uid_path)  # save uid for further analysis\n",
    "        del cnt_vec_jz\n",
    "        del cnt_vec_sk\n",
    "        del cnt_dict\n",
    "        del cnt_vectorizer\n",
    "        gc.collect()\n",
    "\n",
    "    with profiler(\"TFIDF vectorizing (ngram=1) transformation\"):\n",
    "        tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab, \n",
    "                                           tokenizer=tokenizer,\n",
    "                                           dtype=np.float32)\n",
    "        tfidf_vec = tfidf_vectorizer.fit_transform(docs)\n",
    "        \n",
    "\n",
    "    with profiler(\"saving TFIDF vectors\", verbose_time=False):\n",
    "        tfidf_file = \"userFeature.[featureName='{}'].pkl\".format(feat_name)\n",
    "        tfidf_path = os.path.join(tfidf_dir, tfidf_file)\n",
    "        save_as_pickle((tfidf_vectorizer.vocabulary_, tfidf_vectorizer.idf_, tfidf_vec), tfidf_path)  # save mapping and idf as well\n",
    "        if i == 0:\n",
    "            uid_file = \"uid.pkl\"\n",
    "            uid_path = os.path.join(tfidf_dir, uid_file)\n",
    "            save_as_pickle(df['uid'].values, uid_path)  # save uid for further analysis\n",
    "        del tfidf_vec\n",
    "        del tfidf_vectorizer\n",
    "        gc.collect()\n",
    "\n",
    "    with profiler(\"cleaning\", verbose_time=False):\n",
    "        del [[docs, df]]\n",
    "        del vocab\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage at this moment: 81.35MB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(\"Memory usage at this moment: {}\".format(get_memory_str()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
